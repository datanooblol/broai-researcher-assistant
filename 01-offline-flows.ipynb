{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da9c8ea-c7d9-48a9-a141-966ee96eafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4d0d4-5b88-4b33-a04b-0a6bfdf5d2ea",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fc23cc-6821-4be0-9392-51afa0356325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB_NAME = \"./memories.db\"\n",
    "# DB_NAME = \"./storm.db\"\n",
    "# DB_NAME = \"./hipporag.db\"\n",
    "DB_NAME = \"./gpt_review.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cebffb2-2f8b-4dc4-8ec9-8098b5a1f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.experiments.vector_store import DuckVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea7a6e5-35c9-40c3-9321-44799cfd7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.jargon_store import JargonStore, JargonRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e8b892-935a-46b6-a5a7-d94f50f154e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29818/2186289470.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "848fd931-a530-42fd-92ce-506af689368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29818/2466456318.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 174278.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension\n",
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8bff62-db89-4b53-ab67-8b20e7466778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29818/1440006260.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_29818/1440006260.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
      "/tmp/ipykernel_29818/1440006260.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n"
     ]
    }
   ],
   "source": [
    "raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
    "enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
    "longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n",
    "jargon_memory = JargonStore(db_name=DB_NAME, table=\"jargon_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c45030-d236-4f5e-b477-55b9b9768edd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "296506ce-2cfb-4192-b40a-d639f250426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.offline_flow import MemoryFlow, OfflineFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34ed3fad-f210-4b5a-aaf9-fced7f4f986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.context_enricher import ContextEnricher\n",
    "from agents.jargon_extractor import JargonExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3900165-39ff-4062-9406-f4f8299d9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "\n",
    "memory_flow = MemoryFlow(raw_memory, enrich_memory, longterm_memory, jargon_memory)\n",
    "offline_flow = OfflineFlow(\n",
    "    knowledge_flow=memory_flow,\n",
    "    context_enricher=ContextEnricher(),\n",
    "    jargon_extractor=JargonExtractor(),\n",
    "    max_tokens=max_tokens,\n",
    "    overlap=int(max_tokens*0.3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f1c5b44-f27b-45f4-9ca3-1e31c5cd8a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/package/offline_flow.py:57: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: pdf_to_markdown\n",
      "  # with open(source, \"r\") as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded texify model s3://texify/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|██████████| 7/7 [00:05<00:00,  1.33it/s]\n",
      "Running OCR Error Detection: 100%|██████████| 10/10 [00:00<00:00, 60.06it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing tables: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "/home/ec2-user/SageMaker/broai-researcher-assistant/package/offline_flow.py:65: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_markdown\n",
      "  consolidated_chunks = consolidate_markdown(chunks)\n",
      "/home/ec2-user/SageMaker/broai-researcher-assistant/package/offline_flow.py:66: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: consolidate_markdown\n",
      "  sections = get_markdown_sections(consolidated_chunks)\n",
      "/home/ec2-user/SageMaker/broai-researcher-assistant/package/offline_flow.py:67: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: get_markdown_sections\n",
      "  contexts = []\n",
      "/home/ec2-user/SageMaker/broai-researcher-assistant/package/offline_flow.py:73: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_overlap\n",
      "  self.payload.contexts = new_contexts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown headings: max(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [02:54<00:00,  1.00it/s]\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='the extracted jargon, acronym, abbreviation, proper name from the context'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='the evidence of the extracted jargon, acronym, abbreviation, proper name from the context'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "100%|██████████| 175/175 [05:46<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# source = \"./docs/storm.md\"\n",
    "# source = \"./docs/hipporag.md\"\n",
    "source = \"./docs/gpt_review.pdf\"\n",
    "offline_flow.run(source=source, source_type=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a437e-5f23-416e-a3ea-b9defc90e2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai-ra",
   "language": "python",
   "name": "broai-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
