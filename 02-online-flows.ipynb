{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d61a8a4-6a32-4254-80c5-de4388f227a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cd0c3-0ab1-42c1-9644-1f670184673e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04460b5-436c-42de-947f-6479c9200be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"./storm.db\"\n",
    "# DB_NAME = \"./hipporag.db\"\n",
    "DB_NAME = \"./gpt_review.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7bfb94-451a-4ef8-a6e9-ca69453770c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.prompt_management.interface import Persona, Instructions, Examples, Example\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from broai.experiments.bro_agent import BroAgent\n",
    "import json\n",
    "from broai.interface import Context, Contexts\n",
    "from broai.experiments.vector_store import DuckVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90163eda-1ed4-4265-9183-c93a957ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.jargon_store import JargonStore, JargonRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea043c71-d617-4ab9-a351-69d1043f2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15363/2186289470.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fb15de4-b7c4-4a32-8c33-57d4a5588d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15363/2466456318.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 212549.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension\n",
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ee5a28-a41d-4cb6-86db-f3c979438387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15363/1440006260.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_15363/1440006260.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
      "/tmp/ipykernel_15363/1440006260.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n"
     ]
    }
   ],
   "source": [
    "raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
    "enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
    "longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n",
    "jargon_memory = JargonStore(db_name=DB_NAME, table=\"jargon_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce04192-42f7-44c5-a0f7-b09280c5f262",
   "metadata": {},
   "source": [
    "# Agent Flows: \n",
    "- JargonDetector\n",
    "- JargonEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3957a288-0ccf-434c-b70e-27f72397c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.online_flow import OnlineFlow, JargonFlow, KnowledgeFlow, Payload\n",
    "from agents.jargon_detector import JargonDetector\n",
    "from agents.jargon_editor import JargonEditor\n",
    "from agents.query_decomposer import QueryDecomposer\n",
    "from agents.ai_oracle import AIOracle\n",
    "from agents.kb_oracle import KBOracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f276f2b-85af-47da-8621-2da15812d323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15363/1386738580.py:9: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory=DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em),\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = \"./storm.db\"\n",
    "\n",
    "jargon_flow = JargonFlow(\n",
    "    jargon_memory=JargonStore(db_name=DB_NAME, table=\"jargon_memory\"), \n",
    "    jargon_detector=JargonDetector(), \n",
    "    jargon_editor=JargonEditor()\n",
    ")\n",
    "knowledge_flow = KnowledgeFlow(\n",
    "    longterm_memory=DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em), \n",
    "    reranker=rr, \n",
    "    search_method=\"vector\"\n",
    ")\n",
    "olf = OnlineFlow(\n",
    "    oracle=KBOracle(), \n",
    "    knowledge_flow=knowledge_flow,\n",
    "    jargon_flow=jargon_flow,\n",
    "    query_decomposer=QueryDecomposer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3a3467b-844a-4f02-a957-92459b6eac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM stands for Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking.\n",
      "CPU times: user 660 ms, sys: 32.9 ms, total: 693 ms\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# olf.run(message=\"What is the dataset used in the study?\")\n",
    "# olf.run(message=\"What does STORM do in the study?\")\n",
    "olf.run(message=\"What does STORM stand for?\")\n",
    "# olf.run(message=\"What does STORM stand for and how does it work? Explain it in step-by-step manner.\")\n",
    "# olf.run(message=\"What does STORM stand for and how does it work? Could you please explain it like I'm a five years old? I'm really new to this thing.\")\n",
    "\n",
    "print(olf.payload.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "083bc98b-1af1-4ff5-81d5-c96fd87754cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15363/2648505056.py:9: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory=DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em),\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = \"./hipporag.db\"\n",
    "\n",
    "jargon_flow = JargonFlow(\n",
    "    jargon_memory=JargonStore(db_name=DB_NAME, table=\"jargon_memory\"), \n",
    "    jargon_detector=JargonDetector(), \n",
    "    jargon_editor=JargonEditor()\n",
    ")\n",
    "knowledge_flow = KnowledgeFlow(\n",
    "    longterm_memory=DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em), \n",
    "    reranker=rr, \n",
    "    search_method=\"vector\"\n",
    ")\n",
    "olf = OnlineFlow(\n",
    "    oracle=KBOracle(), \n",
    "    knowledge_flow=knowledge_flow,\n",
    "    jargon_flow=jargon_flow,\n",
    "    query_decomposer=QueryDecomposer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb5fd66a-93d3-4eec-938c-c67e3c0e4fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HippoRAG stands for a novel retrieval framework inspired by human long-term memory, specifically the hippocampal indexing theory of human long-term memory.\n",
      "CPU times: user 667 ms, sys: 26.9 ms, total: 694 ms\n",
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "olf.run(message=\"What does HIPPORAG stand for?\")\n",
    "# olf.run(message=\"What does HippoRAG do in the study?\")\n",
    "# olf.run(message=\"What does HippoRAG stand for and how does it work? Explain it in step-by-step manner.\")\n",
    "# olf.run(message=\"What does HippoRAG stand for and how does it work? Could you please explain it like I'm a five years old? I'm really new to this thing.\")\n",
    "\n",
    "print(olf.payload.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "057c3113-3877-4019-bb1a-dd1163e388a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15363/2816527970.py:9: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory=DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em),\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = \"./gpt_review.db\"\n",
    "\n",
    "jargon_flow = JargonFlow(\n",
    "    jargon_memory=JargonStore(db_name=DB_NAME, table=\"jargon_memory\"), \n",
    "    jargon_detector=JargonDetector(), \n",
    "    jargon_editor=JargonEditor()\n",
    ")\n",
    "knowledge_flow = KnowledgeFlow(\n",
    "    longterm_memory=DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em), \n",
    "    reranker=rr, \n",
    "    search_method=\"vector\"\n",
    ")\n",
    "olf = OnlineFlow(\n",
    "    oracle=KBOracle(), \n",
    "    knowledge_flow=knowledge_flow,\n",
    "    jargon_flow=jargon_flow,\n",
    "    query_decomposer=QueryDecomposer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "617b22d5-6534-43c9-8dd1-92c21c93147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the review, GPT, or Generative Pre-trained Transformer, is a type of language model that uses deep learning techniques to generate human-like text based on input. It is a pre-trained language model that can perform a wide range of language-related tasks, including text generation, language translation, and text classification.\n",
      "\n",
      "The review highlights the evolution of GPT models, starting from GPT-1, which was a 12-layer decoder architecture, to GPT-4, which is a more advanced model with 175 billion parameters. The review also discusses the applications of GPT models, including education, healthcare, customer service, and game design.\n",
      "\n",
      "In the context of education, GPT models can assist in identifying leisure activities, job searching, and lifestyle improvement with personalized recommendations. They can also aid in research writing, automated assessments, and fostering creativity.\n",
      "\n",
      "In game design, GPT models can be used for content creation, analyzing player abilities, and generating NPCs. They can create game content, detect and analyze player skills, and generate dialogue for NPCs, enhancing the gaming experience.\n",
      "\n",
      "Overall, the review highlights the potential of GPT models to transform various industries and applications, and provides a comprehensive overview of their architecture, enabling technologies, applications, challenges, and future directions.\n",
      "CPU times: user 644 ms, sys: 36.2 ms, total: 680 ms\n",
      "Wall time: 6.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# olf.run(message=\"What does GPT stand for?\")\n",
    "olf.run(message=\"What does GPT do in the review?\")\n",
    "\n",
    "print(olf.payload.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cb2a9-4d8d-47a2-9830-50fe2f8f2098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai-ra",
   "language": "python",
   "name": "broai-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
