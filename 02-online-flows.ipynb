{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d61a8a4-6a32-4254-80c5-de4388f227a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cd0c3-0ab1-42c1-9644-1f670184673e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04460b5-436c-42de-947f-6479c9200be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"./memories.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7bfb94-451a-4ef8-a6e9-ca69453770c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.prompt_management.interface import Persona, Instructions, Examples, Example\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from broai.experiments.bro_agent import BroAgent\n",
    "import json\n",
    "from broai.interface import Context, Contexts\n",
    "from broai.experiments.vector_store import DuckVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90163eda-1ed4-4265-9183-c93a957ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.jargon_store import JargonStore, JargonRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea043c71-d617-4ab9-a351-69d1043f2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10697/2186289470.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb15de4-b7c4-4a32-8c33-57d4a5588d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10697/2466456318.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 157090.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension\n",
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ee5a28-a41d-4cb6-86db-f3c979438387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10697/1440006260.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_10697/1440006260.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
      "/tmp/ipykernel_10697/1440006260.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n"
     ]
    }
   ],
   "source": [
    "raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
    "enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
    "longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n",
    "jargon_memory = JargonStore(db_name=DB_NAME, table=\"jargon_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce04192-42f7-44c5-a0f7-b09280c5f262",
   "metadata": {},
   "source": [
    "# Agent Flows: \n",
    "- JargonDetector\n",
    "- JargonEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3957a288-0ccf-434c-b70e-27f72397c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.flow import OnlineFlow, JargonFlow, KnowledgeFlow, Payload\n",
    "from agents.jargon_detector import JargonDetector\n",
    "from agents.jargon_editor import JargonEditor\n",
    "from agents.query_decomposer import QueryDecomposer\n",
    "from agents.ai_oracle import AIOracle\n",
    "from agents.kb_oracle import KBOracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94fb78bf-a379-494b-8dcc-f295b8c8c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = Payload(message=\"What does STORM do in the study?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ea56125-d1bc-473d-923b-e4f7851cefb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25f98136-0e7a-4e24-8068-da25f67b4ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f276f2b-85af-47da-8621-2da15812d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargon_flow = JargonFlow(jargon_memory=jargon_memory, jargon_detector=JargonDetector(), jargon_editor=JargonEditor())\n",
    "knowledge_flow = KnowledgeFlow(longterm_memory=longterm_memory, reranker=rr)\n",
    "# olf = OnlineFlow(oracle=AIOracle(), jargon_flow=jargon_flow)\n",
    "olf = OnlineFlow(\n",
    "    oracle=KBOracle(), \n",
    "    knowledge_flow=knowledge_flow,\n",
    "    jargon_flow=jargon_flow,\n",
    "    query_decomposer=QueryDecomposer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b3a3467b-844a-4f02-a957-92459b6eac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# olf.run(message=\"What is the dataset used in the study?\")\n",
    "# olf.run(message=\"What does STORM do in the study?\")\n",
    "# olf.run(message=\"What does STORM stand for?\")\n",
    "# olf.run(message=\"What does STORM stand for and how does it work? Explain it in step-by-step manner.\")\n",
    "olf.run(message=\"What does STORM stand for and how does it work? Could you please explain it like I'm a five years old? I'm really new to this thing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01a7031b-503c-49bc-80ff-ec2a0f40666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me explain STORM in a super simple way.\n",
      "\n",
      "Imagine you want to write a big article about a topic, like dinosaurs. But, you don't know where to start. That's where STORM comes in. STORM is like a special tool that helps you write a great article by doing some of the hard work for you.\n",
      "\n",
      "STORM stands for Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. It's a big phrase, but don't worry, I'll break it down for you.\n",
      "\n",
      "\"Synthesis\" means combining different things together to make something new. In this case, STORM combines different ideas and information to help you write a great article.\n",
      "\n",
      "\"Topic Outlines\" means creating a plan or a map of what you want to write about. STORM helps you create a good plan by asking you questions and finding information about your topic.\n",
      "\n",
      "\"Retrieval\" means finding information from different places, like books or the internet. STORM uses this information to help you write a great article.\n",
      "\n",
      "\"Multi-perspective Question Asking\" means asking questions from different points of view. It's like asking a friend, a teacher, and a scientist about the same topic, and then combining their answers to get a better understanding.\n",
      "\n",
      "So, how does STORM work? Here's a simple example:\n",
      "\n",
      "1. You tell STORM what topic you want to write about, like dinosaurs.\n",
      "2. STORM asks you some questions, like \"What do you want to know about dinosaurs?\" or \"What's your favorite type of dinosaur?\"\n",
      "3. STORM finds information from different places, like books or the internet, to help you answer your questions.\n",
      "4. STORM asks more questions from different points of view, like a paleontologist or a kid who loves dinosaurs.\n",
      "5. STORM combines all the information and answers to create a great plan or outline for your article.\n",
      "6. Finally, STORM helps you write a great article based on your plan.\n",
      "\n",
      "That's STORM in a nutshell! It's like a special tool that helps you write a great article by doing some of the hard work for you.\n"
     ]
    }
   ],
   "source": [
    "print(olf.payload.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99b48d2a-e465-4324-9c90-87523c1c021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Meaning of STORM acronym', 'How STORM works', 'Explanation of STORM in simple terms']\n"
     ]
    }
   ],
   "source": [
    "print(olf.payload.sub_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "850f1c2a-f974-4b99-b734-97454d8b1410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JargonRecord(id='8a50dbfa-a635-42d8-82de-aae082439bc9', jargon='STORM', evidence='we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking', explanation='STORM is short for Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking', metadata={'section': '1 Introduction', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 3}, created_at=Timestamp('2025-04-28 19:03:04.082071')), JargonRecord(id='07824b51-5ae3-42e1-9c7c-356b6ab6fe6c', jargon='STORM', evidence='we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking', explanation='STORM is short for Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking', metadata={'section': '1 Introduction', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 2}, created_at=Timestamp('2025-04-28 19:03:02.162489')), JargonRecord(id='d6b4696b-6574-41b3-9839-f5d1bbd812f2', jargon='STORM', evidence='STORM discovers different perspectives by surveying existing articles from similar topics', explanation='STORM is a system or method for discovering perspectives and controlling question asking process', metadata={'section': '3.1 Perspective-Guided Question Asking', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 9}, created_at=Timestamp('2025-04-28 19:03:13.876676')), JargonRecord(id='898caf7b-7ef3-4155-a8ed-d0d45adbd526', jargon='STORM', evidence='We present STORM to automate the pre-writing stage', explanation='STORM is a system that automates the pre-writing stage', metadata={'section': '3 Method', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 8}, created_at=Timestamp('2025-04-28 19:03:12.026500')), JargonRecord(id='05acc5d0-2ed8-4326-a0a5-d02be5eed257', jargon='STORM', evidence='We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking', explanation='STORM is a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking', metadata={'section': 'Abstract', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 1}, created_at=Timestamp('2025-04-28 19:03:00.348732'))]\n"
     ]
    }
   ],
   "source": [
    "print(olf.payload.jargon_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89234e4b-f2b8-4ee5-83cc-861e92b490f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: STORM\n",
      "Evidence: we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "Explanation: STORM is short for Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "\n",
      "2: STORM\n",
      "Evidence: we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "Explanation: STORM is short for Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "\n",
      "3: STORM\n",
      "Evidence: STORM discovers different perspectives by surveying existing articles from similar topics\n",
      "Explanation: STORM is a system or method for discovering perspectives and controlling question asking process\n",
      "\n",
      "4: STORM\n",
      "Evidence: We present STORM to automate the pre-writing stage\n",
      "Explanation: STORM is a system that automates the pre-writing stage\n",
      "\n",
      "5: STORM\n",
      "Evidence: We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "Explanation: STORM is a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n"
     ]
    }
   ],
   "source": [
    "print(olf.payload.jargon_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b50e0859-475e-490f-8b0a-bdbde6811f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(olf.payload.reranked_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837eb077-a952-4e3d-93f9-60da677104ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai-ra",
   "language": "python",
   "name": "broai-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
