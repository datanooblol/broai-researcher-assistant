{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d61a8a4-6a32-4254-80c5-de4388f227a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cd0c3-0ab1-42c1-9644-1f670184673e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b04460b5-436c-42de-947f-6479c9200be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"./memories.db\"\n",
    "# DB_NAME = \"./test.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea7bfb94-451a-4ef8-a6e9-ca69453770c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.prompt_management.interface import Persona, Instructions, Examples, Example\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from broai.experiments.bro_agent import BroAgent\n",
    "import json\n",
    "from broai.interface import Context, Contexts\n",
    "from broai.experiments.vector_store import DuckVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90163eda-1ed4-4265-9183-c93a957ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.jargon_store import JargonStore, JargonRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea043c71-d617-4ab9-a351-69d1043f2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_437/2186289470.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fb15de4-b7c4-4a32-8c33-57d4a5588d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_437/2466456318.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 171663.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension\n",
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06ee5a28-a41d-4cb6-86db-f3c979438387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_437/1440006260.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_437/1440006260.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
      "/tmp/ipykernel_437/1440006260.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n"
     ]
    }
   ],
   "source": [
    "raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
    "enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
    "longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n",
    "jargon_memory = JargonStore(db_name=DB_NAME, table=\"jargon_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce04192-42f7-44c5-a0f7-b09280c5f262",
   "metadata": {},
   "source": [
    "# Agent Flows: \n",
    "- JargonDetector\n",
    "- JargonEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3957a288-0ccf-434c-b70e-27f72397c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.online_flow import OnlineFlow, JargonFlow, KnowledgeFlow, Payload\n",
    "from agents.jargon_detector import JargonDetector\n",
    "from agents.jargon_editor import JargonEditor\n",
    "from agents.query_decomposer import QueryDecomposer\n",
    "from agents.ai_oracle import AIOracle\n",
    "from agents.kb_oracle import KBOracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f276f2b-85af-47da-8621-2da15812d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargon_flow = JargonFlow(jargon_memory=jargon_memory, jargon_detector=JargonDetector(), jargon_editor=JargonEditor())\n",
    "knowledge_flow = KnowledgeFlow(longterm_memory=longterm_memory, reranker=rr, search_method=\"vector\")\n",
    "olf = OnlineFlow(\n",
    "    oracle=KBOracle(), \n",
    "    knowledge_flow=knowledge_flow,\n",
    "    jargon_flow=jargon_flow,\n",
    "    query_decomposer=QueryDecomposer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3a3467b-844a-4f02-a957-92459b6eac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me explain STORM in a super simple way.\n",
      "\n",
      "Imagine you want to write a big article about a topic, like dinosaurs. But, you don't know where to start. That's where STORM comes in. STORM is like a special tool that helps you write a great article by doing some of the hard work for you.\n",
      "\n",
      "STORM stands for Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. It's a big phrase, but don't worry, I'll break it down for you.\n",
      "\n",
      "\"Synthesis\" means combining different things together to make something new. In this case, STORM combines different ideas and information to help you write a great article.\n",
      "\n",
      "\"Topic Outlines\" means creating a plan or a map of what you want to write about. STORM helps you create a good plan by asking you questions and finding information about your topic.\n",
      "\n",
      "\"Retrieval\" means finding information from different places, like books or the internet. STORM uses this information to help you write a great article.\n",
      "\n",
      "\"Multi-perspective Question Asking\" means asking questions from different points of view. It's like asking a friend, a teacher, and a scientist about the same topic, and then combining their answers to get a better understanding.\n",
      "\n",
      "So, how does STORM work? Here's a simple example:\n",
      "\n",
      "1. You tell STORM what topic you want to write about, like dinosaurs.\n",
      "2. STORM asks you some questions, like \"What do you want to know about dinosaurs?\" or \"What's your favorite dinosaur?\"\n",
      "3. STORM finds information from different places, like books or the internet, to help you answer your questions.\n",
      "4. STORM asks more questions from different points of view, like a paleontologist or a kid who loves dinosaurs.\n",
      "5. STORM combines all the information and answers to create a great plan or outline for your article.\n",
      "6. Finally, STORM helps you write a great article based on your plan.\n",
      "\n",
      "That's STORM in a nutshell! It's like a special tool that helps you write a great article by doing some of the hard work for you.\n",
      "CPU times: user 713 ms, sys: 20.8 ms, total: 734 ms\n",
      "Wall time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# olf.run(message=\"What is the dataset used in the study?\")\n",
    "# olf.run(message=\"What does STORM do in the study?\")\n",
    "# olf.run(message=\"What does STORM stand for?\")\n",
    "# olf.run(message=\"What does STORM stand for and how does it work? Explain it in step-by-step manner.\")\n",
    "olf.run(message=\"What does STORM stand for and how does it work? Could you please explain it like I'm a five years old? I'm really new to this thing.\")\n",
    "\n",
    "print(olf.payload.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83710e65-20df-446d-8921-1a28a1a96e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2, 9, 22, 8, 27, 39, 46, 20]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[context.metadata['sequence'] for context in olf.payload.reranked_contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae4f80f-b62b-4a66-a863-2f1d118b71b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 9, 8, 27, 39, 46, 20, 11, 10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[context.metadata['sequence'] for context in olf.payload.reranked_contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b50e0859-475e-490f-8b0a-bdbde6811f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 20, 22, 9, 27, 8, 10, 46, 39]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[context.metadata['sequence'] for context in olf.payload.reranked_contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bc98b-1af1-4ff5-81d5-c96fd87754cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai-ra",
   "language": "python",
   "name": "broai-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
