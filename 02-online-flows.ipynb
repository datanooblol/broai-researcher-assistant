{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d61a8a4-6a32-4254-80c5-de4388f227a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cd0c3-0ab1-42c1-9644-1f670184673e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04460b5-436c-42de-947f-6479c9200be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"./memories.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7bfb94-451a-4ef8-a6e9-ca69453770c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.prompt_management.interface import Persona, Instructions, Examples, Example\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from broai.experiments.bro_agent import BroAgent\n",
    "import json\n",
    "from broai.interface import Context, Contexts\n",
    "from broai.experiments.vector_store import DuckVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90163eda-1ed4-4265-9183-c93a957ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.jargon_store import JargonStore, JargonRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea043c71-d617-4ab9-a351-69d1043f2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27340/2186289470.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb15de4-b7c4-4a32-8c33-57d4a5588d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27340/2466456318.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 202623.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension\n",
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ee5a28-a41d-4cb6-86db-f3c979438387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27340/1440006260.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_27340/1440006260.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
      "/tmp/ipykernel_27340/1440006260.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n"
     ]
    }
   ],
   "source": [
    "raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
    "enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
    "longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n",
    "jargon_memory = JargonStore(db_name=DB_NAME, table=\"jargon_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce04192-42f7-44c5-a0f7-b09280c5f262",
   "metadata": {},
   "source": [
    "# Agent Flows: \n",
    "- JargonDetector\n",
    "- JargonEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a806cf1-49d3-4483-a24e-515d0d28da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.jargon_detector import JargonDetector, InputMessage\n",
    "from agents.jargon_editor import JargonEditor, InputEditMessage\n",
    "from agents.query_decomposer import QueryDecomposer, InputMessage\n",
    "from agents.context_compressor import ContextCompressor, InputContextCompressor\n",
    "from agents.oracle import Oracle, InputOracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d545c43f-7d0c-47da-be81-c817a45ad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_conversation(original_message, model_name=\"us.meta.llama3-2-11b-instruct-v1:0\"):\n",
    "    potential_jargons = JargonDetector.run(request=InputMessage(message=original_message))\n",
    "    detected_jargons = [j for j in potential_jargons.jargons if j.confidence>.5]\n",
    "    proxy_message = original_message\n",
    "    if len(detected_jargons) > 0:\n",
    "        jargon_knowledges = []\n",
    "        for j in detected_jargons:\n",
    "            jk = jargon_memory.fulltext_search(search_query=\"STORM\")\n",
    "            jargon_knowledges.extend(jk)\n",
    "    \n",
    "        jargon_knowledges_str = \"\\n\\n\".join([f\"{enum+1}: {j.jargon}\\nEvidence: {j.evidence}\\nExplanation: {j.explanation}\" for enum, j in enumerate(jargon_knowledges)])\n",
    "        edited_message = JargonEditor.run(InputEditMessage(knowledge=jargon_knowledges_str, message=original_message))\n",
    "        proxy_message = edited_message.edited_message\n",
    "        \n",
    "    sub_queries = QueryDecomposer.run(InputMessage(message=proxy_message))\n",
    "    retreived_contexts = []\n",
    "    for sq in sub_queries.sub_queries:\n",
    "        rc = longterm_memory.vector_search(search_query=sq, limit=10)\n",
    "        retreived_contexts.extend(rc)\n",
    "    id_list = []\n",
    "    deduplicated_contexts = []\n",
    "    for c in retreived_contexts:\n",
    "        if c.id not in id_list:\n",
    "            id_list.append(c.id)\n",
    "            deduplicated_contexts.append(c)\n",
    "    reranked_contexts, scores = rr.run(search_query=proxy_message, contexts=deduplicated_contexts, top_n=20)\n",
    "    # compressed_contexts = []\n",
    "    # knowledge_contexts = []\n",
    "    # for enum, c in enumerate(reranked_contexts):\n",
    "    #     cc = ContextCompressor.run(InputContextCompressor(context=c.context, query=proxy_message))\n",
    "    #     compressed_contexts.append(cc)\n",
    "    #     for i in cc.extracted_contexts:\n",
    "    #         knowledge_contexts.append(\n",
    "    #             Context(id=c.id, context=i, metadata=c.metadata.copy())\n",
    "    #         )\n",
    "    knowledge_contexts = reranked_contexts\n",
    "    Oracle.model.model_name = model_name\n",
    "    prior_knowledge = \"\\n\\n\".join([f\"{c.context}\" for c in knowledge_contexts if c.context.lower() not in \"error\"])\n",
    "    # answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=\", \".join(sub_queries.sub_queries)))\n",
    "    answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=proxy_message))\n",
    "    return answer, reranked_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495c4c54-1818-40ac-aee0-baa17b68f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PotentialJargon(jargon='STORM', confidence=0.8)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original_message = \"What does STORM do in the research study?\"\n",
    "original_message = \"Explain how STORM works in the study to me like I'm a five years old.\"\n",
    "potential_jargons = JargonDetector.run(request=InputMessage(message=original_message))\n",
    "detected_jargons = [j for j in potential_jargons.jargons if j.confidence>.5]\n",
    "detected_jargons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d6ffe51-458a-49ae-a646-d4964812230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_message = original_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8f498c-6d85-45d5-b523-9dd4cc76599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargon_knowledges = []\n",
    "for j in detected_jargons:\n",
    "    jk = jargon_memory.fulltext_search(search_query=\"STORM\")\n",
    "    jargon_knowledges.extend(jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd089142-3db7-48e4-ba38-55bbba916b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: STORM\n",
      "Evidence: We present STORM to automate the pre-writing stage\n",
      "Explanation: STORM is a system that automates the pre-writing stage\n",
      "\n",
      "2: STORM\n",
      "Evidence: we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "Explanation: STORM is a paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "\n",
      "3: STORM\n",
      "Evidence: STORM simulates a conversation between a Wikipedia writer and a topic expert\n",
      "Explanation: STORM is a system that simulates conversations\n",
      "\n",
      "4: STORM\n",
      "Evidence: STORM discovers different perspectives by surveying existing articles from similar topics\n",
      "Explanation: STORM is a system or method for discovering perspectives and controlling question asking process\n",
      "\n",
      "5: STORM\n",
      "Evidence: We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "Explanation: STORM is a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n"
     ]
    }
   ],
   "source": [
    "jargon_knowledges_str = \"\\n\\n\".join([f\"{enum+1}: {j.jargon}\\nEvidence: {j.evidence}\\nExplanation: {j.explanation}\" for enum, j in enumerate(jargon_knowledges)])\n",
    "print(jargon_knowledges_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2cfa7d2-d33c-41be-a1fd-a4d442a27ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_message = JargonEditor.run(InputEditMessage(knowledge=jargon_knowledges_str, message=original_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afa54ae7-85ac-424d-9e85-7a51b67935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(jargon_knowledges)>0:\n",
    "    proxy_message = edited_message.edited_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2c70d0c-249b-4272-8196-6a17b914f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain how STORM works in the study to me like I'm a five years old.\n",
      "==========\n",
      "Explain how STORM, a system that automates the pre-writing stage, works in the study, considering it is a paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, and it simulates conversations between a writer and an expert, discovers different perspectives, and controls the question asking process, in a way that a five-year-old can understand.\n"
     ]
    }
   ],
   "source": [
    "print(original_message)\n",
    "print(\"=\"*10)\n",
    "print(proxy_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2362d58-ee28-4668-b300-751c79df3d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecomposedQueries(sub_queries=['How STORM works in a study', 'Explaining STORM to a 5-year-old'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryDecomposer.run(InputMessage(message=original_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a56228a3-7501-4a41-9f57-5ef82d274d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How STORM automates the pre-writing stage',\n",
       " 'How STORM simulates conversations between a writer and an expert',\n",
       " 'How STORM discovers different perspectives',\n",
       " 'How STORM controls the question asking process',\n",
       " 'Explain STORM in simple terms for a five-year-old']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_queries = QueryDecomposer.run(InputMessage(message=proxy_message))\n",
    "sub_queries.sub_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e92889-96d6-4595-8a33-ef981676141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_contexts = []\n",
    "for sq in sub_queries.sub_queries:\n",
    "    rc = longterm_memory.vector_search(search_query=sq, limit=10)\n",
    "    retreived_contexts.extend(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d766e7b-fe0c-444e-b1c1-126e2907ff9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retreived_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0caf2fc6-3959-4381-ae9e-dedbebd26927",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "deduplicated_contexts = []\n",
    "for c in retreived_contexts:\n",
    "    if c.id not in id_list:\n",
    "        id_list.append(c.id)\n",
    "        deduplicated_contexts.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a80cf0-2343-405b-bf09-c0d0d08c48dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retreived_contexts), len(deduplicated_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47e2edf6-49ba-4b33-87e1-9a9a9e8ad21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_contexts, scores = rr.run(search_query=proxy_message, contexts=deduplicated_contexts, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaeef759-d6a8-4909-8b34-05b7250a2748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STORM is a system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article.\\n\\n<span id=\"page-2-8\"></span>3 Method\\n\\nWe present STORM to automate the pre-writing stage by researching a given topic via effective question asking ([§3.1,](#page-3-0) [§3.2\\\\)](#page-3-1) and creating an outline ([§3.3\\\\)](#page-4-0). The outline will be extended to a fulllength article grounded on the collected references\\n\\n<span id=\"page-2-2\"></span><sup>2</sup> In practice, S also includes organizational elements such as section and subsection titles, which do not require citations.\\n\\n<span id=\"page-2-3\"></span><sup>3</sup>Obtained from [https://wikimedia.](https://wikimedia.org/api/rest_v1/metrics/edited-pages/top-by-edits/en.wikipedia/all-editor-types/content/) [org/api/rest\\\\\\\\_v1/metrics/edited-pages/](https://wikimedia.org/api/rest_v1/metrics/edited-pages/top-by-edits/en.wikipedia/all-editor-types/content/) [top-by-edits/en.wikipedia/all-editor-types/](https://wikimedia.org/api/rest_v1/metrics/edited-pages/top-by-edits/en.wikipedia/all-editor-types/content/)\\n\\n[content/](https://wikimedia.org/api/rest_v1/metrics/edited-pages/top-by-edits/en.wikipedia/all-editor-types/content/){year}/{month}/all-days\\n\\n<span id=\"page-2-4\"></span><sup>4</sup> <https://www.mediawiki.org/wiki/ORES>\\n\\n<span id=\"page-2-5\"></span><sup>5</sup> [https://en.wikipedia.org/wiki/Wikipedia:](https://en.wikipedia.org/wiki/Wikipedia:Stand-alone_lists) [Stand-alone\\\\\\\\_lists](https://en.wikipedia.org/wiki/Wikipedia:Stand-alone_lists)\\n\\n<span id=\"page-2-6\"></span><sup>6</sup> Since language models process and produce sequences, we can linearize O by adding \"#\" to indicate section titles, \"##\" to indicate subsection titles, etc.\\n\\n<span id=\"page-3-2\"></span>![](_page_3_Figure_0.jpeg)\\n\\nFigure 2: The overview of STORM that automates the pre-writing stage. Starting with a given topic, STORM identifies various perspectives on covering the topic by surveying related Wikipedia articles ( 1 - 2 ). It then simulates conversations between a Wikipedia writer who asks questions guided by the given perspective and an expert grounded on trustworthy online sources ( 3 - 6 ). The final outline is curated based on the LLM\\'s intrinsic knowledge and the gathered conversations from different perspectives ( 7 - 8 ).\\n\\n([§3.4\\\\)](#page-4-1). Figure [2](#page-3-2) gives an overview of STORM and we include the pseudo code in Appendix [B.](#page-13-2)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_contexts[0].context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "241765f0-bf2c-4fd1-8cbf-f8fcee796ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ContextCompressor.model.model_name = \"us.meta.llama3-2-11b-instruct-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "004d5069-54b9-487f-8c29-cb22209d70dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for ExtractedContext\n",
      "  Invalid JSON: invalid escape at line 7 column 81 [type=json_invalid, input_value='\\n{\\n    \"extracted_cont...le domain.\"\\n    ]\\n}\\n', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n",
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for ExtractedContext\n",
      "  Invalid JSON: trailing characters at line 13 column 1 [type=json_invalid, input_value='\\n{\\n    \"extracted_cont...n outline.\"\\n    ]\\n}\\n', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "compressed_contexts = []\n",
    "knowledge_contexts = []\n",
    "for enum, c in enumerate(reranked_contexts[:]):\n",
    "    cc = ContextCompressor.run(InputContextCompressor(context=c.context, query=\", \".join(sub_queries.sub_queries)))\n",
    "    compressed_contexts.append(cc)\n",
    "    for i in cc.extracted_contexts:\n",
    "        knowledge_contexts.append(\n",
    "            Context(id=c.id, context=i, metadata=c.metadata.copy())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffb57247-2642-4dd2-bbcf-9cfb59a70fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compressed_contexts), len(knowledge_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b34c9b2f-c5fc-4769-93f0-a55d27742b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STORM, an LLM-based writing system, automates pre-writing stage for creating Wikipedia-like articles, improving outline and article quality.',\n",
       " 'Experimental results demonstrate that the question asking mechanism in STORM improves both the outline and article quality.',\n",
       " 'error',\n",
       " 'error',\n",
       " 'error',\n",
       " 'We propose STORM, an LLM-based writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_contexts[1].extracted_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9953ed3-119f-4d8d-b7b1-7aea82464511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'error'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_contexts[12].context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21a4aaa7-ec7e-4976-ad5d-79a2ab167211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM is a system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article.\n",
      "\n",
      "We present STORM to automate the pre-writing stage by researching a given topic via effective question asking and creating an outline.\n",
      "\n",
      "The outline will be extended to a fulllength article grounded on the collected references\n",
      "\n",
      "S also includes organizational elements such as section and subsection titles, which do not require citations.\n",
      "\n",
      "Obtained from https://wikimedia.org/api/rest_v1/metrics/edited-pages/top-by-edits/en.wikipedia/all-editor-types/content/\n",
      "\n",
      "Since language models process and produce sequences, we can linearize O by adding \"#\" to indicate section titles, \"##\" to indicate subsection titles, etc.\n",
      "\n",
      "It then simulates conversations between a Wikipedia writer who asks questions guided by the given perspective and an expert grounded on trustworthy online sources\n",
      "\n",
      "The final outline is curated based on the LLM's intrinsic knowledge and the gathered conversations from different perspectives\n",
      "\n",
      "STORM identifies various perspectives on covering the topic by surveying related Wikipedia articles\n",
      "\n",
      "STORM, an LLM-based writing system, automates pre-writing stage for creating Wikipedia-like articles, improving outline and article quality.\n",
      "\n",
      "Experimental results demonstrate that the question asking mechanism in STORM improves both the outline and article quality.\n",
      "\n",
      "We propose STORM, an LLM-based writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch.\n",
      "\n",
      "It proposes a novel system called STORM that automates the pre-writing stage by researching the topic and creating an outline using LLMs to ask incisive questions and retrieving trusted information from the Internet.\n",
      "\n",
      "STORM employs a novel multi-stage approach. It first discovers diverse perspectives by retrieving and analyzing Wikipedia articles from similar topics and then personifies the LLM with specific perspectives for question asking (Figure [1](#page-0-0) (B)).\n",
      "\n",
      "Next, to elicit follow-up questions for iterative research (Figure [1](#page-0-0) (C)), STORM simulates multi-turn conversations where the answers to the generated questions are grounded on the Internet.\n",
      "\n",
      "STORM creates an outline that can be expanded section by section to develop a full-length Wikipedia-like article.\n",
      "\n",
      "STORM researches the topic via perspective-guided question asking in simulated conversations.\n",
      "\n",
      "STORM personifies the LLM with specific perspectives for question asking.\n",
      "\n",
      "STORM simulates multi-turn conversations where the answers to the generated questions are grounded on the Internet.\n",
      "\n",
      "STORM uses LLMs to ask incisive questions and retrieving trusted information from the Internet.\n",
      "\n",
      "STORM, by asking effective questions to research the topic, can create higher recall outlines that cover more topic-specific aspects.\n",
      "\n",
      "STORM discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process.\n",
      "\n",
      "STORM prompts an LLM to generate a list of related topics and subsequently extracts the tables of contents from their corresponding Wikipedia articles, if such articles can be obtained through Wikipedia API.\n",
      "\n",
      "STORM discovers different perspectives by surveying articles and uses them to control question asking, prompting an LLM to generate a list of related topics and extract tables of contents to identify N perspectives that contribute to a comprehensive article.\n",
      "\n",
      "Given the input topic t, STORM discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process.\n",
      "\n",
      "STORM prompts LLMs to ask effective questions by discovering specific perspectives and simulating multi-turn conversations.\n",
      "\n",
      "We conduct the ablation study on outline creation by comparing STORM with two variants: (1) \"STORM w/o Perspective\", which omits perspective in the question generation prompt; (2) \"STORM w/o Conversation\", which prompts LLMs to generate a set number of questions altogether.\n",
      "\n",
      "To ensure a fair comparison, we control an equal total number of generated questions across all variants.\n",
      "\n",
      "STORM w/o Conversation gives much worse results, indicating reading relevant information is crucial to generating effective questions.\n",
      "\n",
      "We further examine how many unique sources are collected in R via different variants. As shown in Table [5,] the full pipeline discovers more different sources and the trend is in accord with the automatic metrics for outline quality.\n",
      "\n",
      "Removing the outline stage significantly deteriorates the performance across all metrics.\n",
      "\n",
      "STORM creates an outline before the actual writing starts.\n",
      "\n",
      "To fully leverage the internal knowledge of LLMs, we first prompt the model to generate a draft outline O<sup>D</sup> given only the topic t.\n",
      "\n",
      "Subsequently, the LLM is prompted with the topic t, the draft outline OD, and the simulated conversations {C0, C1, ..., C<sup>N</sup> } to refine the outline.\n",
      "\n",
      "N + 1 simulated conversations, denoted as {C0, C1, ..., C<sup>N</sup> },\n",
      "\n",
      "1 P0 = \"basic fact writer ...\" // Constant.\n",
      "\n",
      "11 P ← [P0] + P[:N]\n",
      "\n",
      "14 convos ← [ ]\n",
      "\n",
      "15 foreach p in P do\n",
      "\n",
      "16 convo_history ← [ ]\n",
      "\n",
      "19 q ← gen_qn(t, p, dlg_history)\n",
      "\n",
      "22 queries ← gen_queries(t, q)\n",
      "\n",
      "23 sources ← search_and_sift(queries)\n",
      "\n",
      "24 a ← gen_ans(t, q, sources)\n",
      "\n",
      "25 convo_history.append(a)\n",
      "\n",
      "26 R.append(sources)\n",
      "\n",
      "27 end\n",
      "\n",
      "28 convos.append(convo_history)\n",
      "\n",
      "30 // Create the outline.\n",
      "\n",
      "31 OD ← direct_gen_outline(t)\n",
      "\n",
      "32 O ← refine_outline(t, OD, convos)\n"
     ]
    }
   ],
   "source": [
    "prior_knowledge = \"\\n\\n\".join([f\"{c.context}\" for c in knowledge_contexts if c.context.lower() not in \"error\"])\n",
    "print(prior_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2acd700-513e-4a5e-bc20-88e3b0522f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me explain how STORM works in a way that's easy to understand.\n",
      "\n",
      "Imagine you want to write a big article about a topic, like a book. But, you don't know where to start. That's where STORM comes in. STORM is like a super smart assistant that helps you write a great article.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **STORM finds different ideas**: STORM looks at other articles that are similar to the one you want to write. It reads those articles and finds different ideas and perspectives on the topic.\n",
      "2. **STORM asks questions**: STORM then asks questions to an expert (like a super smart computer) to get more information about the topic. It asks questions like \"What's the most important thing to know about this topic?\" or \"Can you give me an example of this?\"\n",
      "3. **STORM talks to the expert**: STORM has a conversation with the expert, and the expert answers the questions. STORM writes down the answers and uses them to create an outline for the article.\n",
      "4. **STORM creates an outline**: The outline is like a map of the article. It shows what the article will be about, what points will be covered, and how they will be organized.\n",
      "5. **STORM writes the article**: Finally, STORM uses the outline to write the article. It's like a super smart writer that can write a great article all by itself!\n",
      "\n",
      "STORM is like a magic tool that helps you write a great article by finding different ideas, asking questions, talking to an expert, and creating an outline. It's like having a super smart assistant that can help you write a great article!\n"
     ]
    }
   ],
   "source": [
    "# Oracle.model.model_name = \"us.meta.llama3-2-3b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=proxy_message))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b8cfe34-51d0-493c-9b8a-9ccb1551f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM simulates conversations between a Wikipedia writer and a topic expert, discovers different perspectives by surveying articles, and uses these perspectives to control question asking, prompting an LLM to generate a list of related topics and extract tables of contents to identify N perspectives that contribute to a comprehensive article.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-2-11b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad216660-2900-4b08-b747-075520367fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM simulates conversations between a Wikipedia writer and a topic expert, discovers different perspectives by surveying articles, and creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f114621-10b5-4f48-b370-45e75dc281e1",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a47af6-b519-4f3e-9e22-aaaf4b766d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    # \"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "    \"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1000e7fc-4266-423d-afe7-04b62e4cdd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It proposes a novel multi-stage approach to research a given topic, create an outline, and extend it to a full-length article grounded on collected references. STORM uses perspectives to guide question asking in the writing process, discovering different viewpoints by surveying articles and controlling the question asking process to create a comprehensive article.\n",
      "\n",
      "STORM's main contributions include:\n",
      "\n",
      "1. Evaluating the capacity of LLM systems at generating long-form grounded articles from scratch, and the pre-writing challenge in particular.\n",
      "2. Proposing STORM, a novel system that automates the pre-writing stage by researching the topic and creating an outline using LLMs to ask incisive questions and retrieving trusted information from the Internet.\n",
      "3. Demonstrating the effectiveness of STORM through both automatic and human evaluation, and revealing new challenges in generating grounded long-form articles.\n",
      "\n",
      "STORM's design is based on two hypotheses: (1) diverse perspectives lead to varied questions, and (2) formulating in-depth questions requires iterative research. It employs a novel multi-stage approach, including discovering diverse perspectives, simulating conversations, and creating an outline.\n",
      "\n",
      "STORM's performance is evaluated using the FreshWiki dataset, which curates recent, high-quality Wikipedia articles. The results show that STORM outperforms the baseline approaches in terms of recall, precision, and human evaluation, with room for improvement.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, is proposed to generate long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. In the research study, STORM is used to automate the pre-writing stage by researching a given topic, creating an outline, and extending it to a full-length article grounded on collected references. STORM discovers diverse perspectives on a topic, simulates conversations between a Wikipedia writer and a topic expert, and creates an outline based on the collected information. The system is evaluated using a dataset of recent high-quality Wikipedia articles and feedback from experienced Wikipedia editors, and is compared to other approaches such as RAG and oRAG. The results show that STORM outperforms these approaches in terms of outline quality, article quality, and human evaluation, but also identifies new challenges for generating grounded long-form articles, such as source bias transfer and over-association of unrelated facts.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"What does STORM do in the research study?\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45fc33ba-b09e-4000-805d-e14cf12f69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "The dataset used in the study of STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, is called FreshWiki. It is a curated dataset of recent high-quality Wikipedia articles, which is used to evaluate the effectiveness of STORM in generating long-form articles from scratch.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "The dataset used in the study of STORM is called FreshWiki, which consists of recent, high-quality Wikipedia articles. It was curated to evaluate the performance of STORM in generating long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. The dataset is used to assess the pre-writing stage and the quality of the generated outlines and articles.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"What does the dataset used in the study?\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "359be844-d9ed-471c-bd22-b73da45edf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage by researching a given topic, creating an outline, and extending it to a full-length article grounded on collected references. It uses a novel multi-stage approach to discover diverse perspectives, simulate multi-turn conversations, and create a comprehensive outline. The system is based on two hypotheses: (1) diverse perspectives lead to varied questions, and (2) formulating in-depth questions requires iterative research.\n",
      "\n",
      "STORM works by first discovering diverse perspectives by retrieving and analyzing Wikipedia articles from similar topics. It then personifies the LLM with specific perspectives for question asking. Next, it simulates multi-turn conversations where the answers to the generated questions are grounded on the Internet. Finally, based on the LLM's internal knowledge and the collected information, STORM creates an outline that can be expanded section by section to develop a full-length Wikipedia-like article.\n",
      "\n",
      "The study evaluates STORM using the FreshWiki dataset, which curates recent, high-quality Wikipedia articles to avoid data leakage during pretraining. The results show that STORM outperforms the baseline RAG and oRAG approaches in terms of recall, precision, and human evaluation. The system also demonstrates the effectiveness of its approach in generating grounded long-form articles.\n",
      "\n",
      "STORM's key components include:\n",
      "\n",
      "1. Perspective-guided question asking: STORM discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process.\n",
      "2. Simulating conversations: STORM simulates a conversation between a Wikipedia writer and a topic expert to generate questions and answers.\n",
      "3. Creating the article outline: STORM creates an outline before the actual writing starts by refining the draft outline generated by the LLM.\n",
      "\n",
      "The study also conducts ablation studies to evaluate the importance of each component. The results show that removing the perspective-guided question asking or conversation simulation components significantly deteriorates the performance of STORM.\n",
      "\n",
      "Overall, STORM is a powerful writing system that can automate the pre-writing stage and generate high-quality outlines and full-length articles. Its ability to discover diverse perspectives, simulate conversations, and create comprehensive outlines makes it a valuable tool for researchers and writers.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a system that automates the pre-writing stage by researching a given topic, creating an outline, and extending it to a full-length article grounded on collected references. It works by discovering diverse perspectives on a topic, simulating conversations between a Wikipedia writer and a topic expert, and using the simulated conversations to create a comprehensive outline. The system uses large language models (LLMs) to ask effective questions, retrieve trusted information from the Internet, and generate answers to these questions. The outline is then refined and expanded into a full-length article. STORM's design is based on two hypotheses: that diverse perspectives lead to varied questions and that formulating in-depth questions requires iterative research. The system has been evaluated using a dataset of recent, high-quality Wikipedia articles and has been shown to outperform other approaches, such as retrieval-augmented generation (RAG), in terms of outline quality and article generation.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"How does STORM work in the study?\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d6deaa0-3903-454c-8500-a1241f68ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It uses a novel multi-stage approach to research a given topic, create an outline, and extend it to a full-length article grounded on collected references. The system is based on two hypotheses: (1) diverse perspectives lead to varied questions, and (2) formulating in-depth questions requires iterative research.\n",
      "\n",
      "STORM employs a multi-stage approach to research the topic, which includes:\n",
      "\n",
      "1. Discovering diverse perspectives by retrieving and analyzing Wikipedia articles from similar topics.\n",
      "2. Simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources.\n",
      "3. Curating the collected information to create an outline.\n",
      "\n",
      "The system uses a novel multi-perspective question asking mechanism to generate questions that are grounded on the Internet and simulate multi-turn conversations to elicit follow-up questions for iterative research.\n",
      "\n",
      "STORM is evaluated using the FreshWiki dataset, which curates recent, high-quality Wikipedia articles to avoid data leakage during pretraining. The evaluation metrics include outline coverage, article quality, and human evaluation. The results show that STORM outperforms the baseline approaches in terms of recall, precision, and human evaluation, with room for improvement.\n",
      "\n",
      "The study also conducts ablation studies to examine the importance of each stage in the STORM pipeline. The results show that removing the outline stage significantly deteriorates the performance across all metrics, indicating that the outline stage is crucial for generating high-quality articles.\n",
      "\n",
      "Overall, STORM is a novel writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It uses a multi-stage approach to research a given topic, create an outline, and extend it to a full-length article grounded on collected references. The system is evaluated using a variety of metrics, including outline coverage, article quality, and human evaluation, and outperforms baseline approaches in terms of recall, precision, and human evaluation.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It works by discovering diverse perspectives on a given topic, simulating conversations between a Wikipedia writer and a topic expert, and creating an outline based on the collected information. \n",
      "\n",
      "The process starts with discovering different perspectives by surveying existing articles from similar topics and using these perspectives to control the question-asking process. The system then simulates multi-turn conversations where the answers to the generated questions are grounded on trustworthy Internet sources. \n",
      "\n",
      "After researching the topic through these conversations, STORM creates an outline by first prompting a language model to generate a draft outline given only the topic, and then refining the outline using the simulated conversations. The final outline is used to produce a full-length article.\n",
      "\n",
      "STORM's design is based on two hypotheses: that diverse perspectives lead to varied questions, and that formulating in-depth questions requires iterative research. The system employs a novel multi-stage approach, using language models to ask effective questions, retrieve trusted information, and create a comprehensive outline. \n",
      "\n",
      "The study evaluates STORM's effectiveness through a dataset of recent high-quality Wikipedia articles and expert feedback from experienced Wikipedia editors. The results show that STORM outperforms other approaches, such as Retrieval-Augmented Generation (RAG), in terms of outline quality, article quality, and human evaluation. However, the study also identifies challenges for future research, including addressing source bias transfer and over-association of unrelated facts.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study.\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cd3fff9-fd86-4644-a894-2e92d9c57bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that helps generate long-form articles from scratch. It's like a research assistant that helps you find the right information and organize it in a way that makes sense. Here's how it works:\n",
      "\n",
      "1. **Discovering perspectives**: STORM starts by finding different perspectives on a topic. It looks at related Wikipedia articles and extracts the tables of contents to get a sense of what's already been written about the topic.\n",
      "2. **Simulating conversations**: STORM then simulates conversations between a Wikipedia writer and a topic expert. The writer asks questions, and the expert provides answers based on trusted sources from the Internet.\n",
      "3. **Creating an outline**: After the conversations, STORM creates an outline for the article. It uses the information gathered to create a draft outline, and then refines it based on the conversations.\n",
      "4. **Writing the article**: Finally, STORM uses the outline and the references collected to write the full-length article. It generates each section separately, using relevant information from the references and the LLM generation.\n",
      "\n",
      "STORM is designed to help researchers and writers generate high-quality articles from scratch. It's a tool that can help you organize your thoughts, find the right information, and write a well-structured article.\n",
      "\n",
      "In the study, STORM was compared to three other approaches: Direct Gen, RAG, and oRAG. The results showed that STORM outperformed the other approaches in terms of recall, precision, and human evaluation. The human evaluators found that STORM-generated articles were more organized, had better coverage, and were more interesting than the other approaches.\n",
      "\n",
      "However, the study also found that STORM struggled with verifiability issues, such as red herring fallacy and overspeculation. This means that STORM sometimes introduced unverifiable connections between different pieces of information or between the information and the topic. Addressing these issues is an area for future research.\n",
      "\n",
      "Overall, STORM is a powerful tool for generating long-form articles from scratch. It's a great resource for researchers and writers who want to produce high-quality articles quickly and efficiently.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system designed to generate long-form articles, similar to those found on Wikipedia. It works by automating the pre-writing stage, which involves researching a topic, creating an outline, and then using that outline to write a full-length article.\n",
      "\n",
      "Here's a step-by-step explanation of how STORM works:\n",
      "\n",
      "1. **Discovering perspectives**: STORM starts by discovering different perspectives on a given topic. It does this by analyzing related Wikipedia articles and identifying the tables of contents, which provide a general outline of the topic.\n",
      "2. **Simulating conversations**: STORM then simulates conversations between a Wikipedia writer and a topic expert. The writer asks questions based on the topic and the expert provides answers grounded in trusted internet sources. This conversation helps to gather more information and clarify the topic.\n",
      "3. **Creating an outline**: After the conversation, STORM creates an outline for the article. It does this by prompting a language model to generate a draft outline, and then refining it based on the information gathered during the conversation.\n",
      "4. **Writing the article**: With the outline in place, STORM uses the language model to generate the full-length article, section by section. It retrieves relevant documents from the internet, generates text based on those documents, and then edits the text for coherence.\n",
      "5. **Evaluation**: Finally, STORM evaluates the quality of the generated article using metrics such as heading soft recall, entity recall, and human evaluation.\n",
      "\n",
      "The goal of STORM is to generate high-quality, well-organized articles that are similar to those found on Wikipedia. By automating the pre-writing stage and using a combination of language models and trusted internet sources, STORM is able to produce articles that are informative, engaging, and well-structured.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study in plain English.\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eb8154a-591a-4401-8559-ef4ea05a3c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "So, you know how sometimes we need to write a big report or an article about something, and it can be really hard to know where to start? That's where STORM comes in. STORM is a special tool that helps us write those big reports and articles by breaking it down into smaller, more manageable pieces.\n",
      "\n",
      "First, STORM helps us figure out what we want to write about. It looks at lots of different sources, like books and websites, to get a good understanding of the topic. Then, it asks us questions to help us think about what we want to say. It's like having a conversation with a friend, but instead of talking, we're writing!\n",
      "\n",
      "Next, STORM helps us organize our thoughts into a plan. It creates an outline, which is like a map of what we want to say. This helps us make sure we cover all the important points and that our writing makes sense.\n",
      "\n",
      "Finally, STORM helps us write the actual article. It uses the outline we created to guide us, and it even suggests some words and phrases to help us get started.\n",
      "\n",
      "The really cool thing about STORM is that it can help us write articles that are just as good as ones written by experts. It's like having a magic writing assistant that can help us create amazing content!\n",
      "\n",
      "But, just like how we need to practice our writing skills, STORM also needs to be trained and improved. The researchers who created STORM are still working on making it even better, so that it can help more people write amazing articles and reports.\n",
      "\n",
      "So, that's STORM in a nutshell! It's a super cool tool that can help us write big reports and articles by breaking it down into smaller pieces, asking us questions, creating an outline, and even suggesting words and phrases to get us started.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "Imagine you want to write a story about a big topic, like dinosaurs. But, you don't know where to start. That's where STORM comes in. It's like a special helper that makes writing easier.\n",
      "\n",
      "STORM does three main things:\n",
      "\n",
      "1. **It finds different ways to look at the topic**: Just like how you might ask your friends what they think about dinosaurs, STORM asks different \"friends\" (called perspectives) what they think about the topic. This helps STORM get lots of ideas.\n",
      "\n",
      "2. **It asks questions and gets answers**: STORM then asks these \"friends\" questions about the topic, like \"What did dinosaurs eat?\" or \"How big were they?\" And it gets answers from trusted sources, like books or websites.\n",
      "\n",
      "3. **It makes an outline**: After asking all these questions and getting answers, STORM makes a special plan called an outline. This outline is like a map that shows what the story will be about and what order it will go in.\n",
      "\n",
      "STORM uses this outline to help write the story. It's like having a special guide that makes sure the story is interesting, organized, and has lots of good information.\n",
      "\n",
      "So, STORM is like a helper that makes writing easier by finding different ideas, asking questions, and making a plan. And that's how it works!\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study to me like I'm a five years old.\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7b9af7c-d5a5-4454-ab6e-c9e16584766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM, the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, is a writing system that automates the pre-writing stage for creating Wikipedia-like articles. The system works in the following steps:\n",
      "\n",
      "1. **Discovering different perspectives**: STORM discovers different perspectives on a given topic by surveying related Wikipedia articles. It extracts the tables of contents from these articles and concatenates them to create a context to prompt the LLM to identify N perspectives that can collectively contribute to a comprehensive article on the topic.\n",
      "\n",
      "2. **Simulating conversations**: STORM simulates conversations between a Wikipedia writer and a topic expert. In each round of the conversation, the LLM-powered Wikipedia writer generates a single question based on the topic, its assigned perspective, and the conversation history. The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions.\n",
      "\n",
      "3. **Question asking and answering**: STORM uses trusted sources from the Internet to ground the answer to each query. It first prompts the LLM to break down the query into a set of search queries, and then searches and sifts the results using a rule-based filter according to the Wikipedia guideline. Finally, the LLM synthesizes the trustworthy sources to generate the answer.\n",
      "\n",
      "4. **Creating the outline**: After thoroughly researching the topic through N + 1 simulated conversations, STORM creates an outline before the actual writing starts. It prompts the LLM to generate a draft outline given only the topic, and then refines the outline using the simulated conversations.\n",
      "\n",
      "5. **Refining the outline**: STORM refines the outline by incorporating the information gathered from the simulated conversations. It uses the LLM's internal knowledge and the gathered conversations from different perspectives to create a comprehensive outline.\n",
      "\n",
      "6. **Generating the article**: STORM generates the full-length article grounded on the collected references. It uses the outline and the references to create a well-structured and informative article.\n",
      "\n",
      "STORM's role in automating the pre-writing stage is to research the topic, create an outline, and extend it to a full-length article grounded on collected references. The system uses LLMs and trusted sources to ensure factual information and to generate effective questions and answers. The pseudo code of STORM is provided in Appendix B, and the implementation details are discussed in Section 4.4.\n",
      "\n",
      "STORM's effectiveness is evaluated using automatic metrics such as ROUGE-1, ROUGE-L, and Entity Recall, as well as human evaluation. The human evaluation study involves 10 experienced Wikipedia editors who rate the articles generated by STORM and compare them with human-written articles. The results show that STORM outperforms the oRAG baseline in terms of breadth and depth, but struggles with verifiability issues such as red herring fallacy and overspeculation.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM, the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, is a framework designed to automate the pre-writing stage of generating long-form articles. The system plays a crucial role in researching a given topic, creating an outline, and extending it to a full-length article grounded on collected references.\n",
      "\n",
      "The process begins with discovering different perspectives on a topic by surveying related Wikipedia articles. STORM prompts a large language model (LLM) to generate a list of related topics and extracts the tables of contents from their corresponding Wikipedia articles. These tables of contents are concatenated to create a context that prompts the LLM to identify various perspectives that can contribute to a comprehensive article on the topic.\n",
      "\n",
      "STORM then simulates conversations between a Wikipedia writer and a topic expert, where the writer asks questions guided by the given perspective, and the expert provides answers grounded on trustworthy online sources. The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions. To ensure the conversation history provides factual information, STORM uses trusted sources from the Internet to ground the answer to each query.\n",
      "\n",
      "The system creates an outline for the article by first generating a draft outline from the topic and then refining it with the simulated conversations. The refined outline is used to produce the full-length article. STORM's approach improves both the outline and article quality, exhibiting greater breadth and depth compared to other baselines.\n",
      "\n",
      "The evaluation of STORM involves automatic metrics, such as ROUGE-1, ROUGE-L, and Entity Recall, as well as human evaluation by experienced Wikipedia editors. The results show that STORM outperforms other approaches, with editors judging articles produced by STORM as more interesting, organized, and having broader coverage. However, STORM also faces challenges, such as verifiability issues, including red herring fallacy and overspeculation, and transferring bias and tone from Internet sources to the generated article.\n",
      "\n",
      "Overall, STORM is a novel system that automates the pre-writing stage by researching a topic, creating an outline, and extending it to a full-length article. Its ability to discover different perspectives and control the question asking process makes it a valuable tool for generating high-quality, long-form articles.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study to me in detail.\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20611cd4-e220-4111-9d65-a3c0fb168322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "The study uses the FreshWiki dataset, which is a curated dataset of recent high-quality Wikipedia articles. The dataset is created to avoid data leakage during pretraining and is used to evaluate the effectiveness of the STORM system. The dataset is also used to establish evaluation criteria for both outline and final article quality.\n",
      "\n",
      "In addition to the FreshWiki dataset, the study also uses a dataset of 100 samples from the FreshWiki dataset with human-written articles under 3000 words for comparison. The dataset is randomly selected and used to evaluate the performance of the STORM system.\n",
      "\n",
      "The study also mentions the use of other datasets and resources, such as the Wikipedia API, to retrieve and analyze Wikipedia articles and to generate questions and answers. However, the primary dataset used in the study is the FreshWiki dataset.\n",
      "\n",
      "The FreshWiki dataset is created by curating recent high-quality Wikipedia articles to avoid data leakage during pretraining. The dataset is used to evaluate the effectiveness of the STORM system and to establish evaluation criteria for both outline and final article quality.\n",
      "\n",
      "The dataset is also used to evaluate the performance of the STORM system in generating outlines and final articles. The study uses metrics such as heading soft recall and heading entity recall to evaluate the quality of the outlines generated by the STORM system.\n",
      "\n",
      "The study also mentions the use of other metrics, such as ROUGE-1, ROUGE-L, and Entity Recall, to evaluate the quality of the final articles generated by the STORM system. However, the primary dataset used in the study is the FreshWiki dataset.\n",
      "\n",
      "In terms of the creation of the dataset, the study does not provide detailed information on how the FreshWiki dataset is created. However, it is mentioned that the dataset is curated to avoid data leakage during pretraining and to ensure that the articles are of high quality.\n",
      "\n",
      "Overall, the study uses a combination of datasets and resources to evaluate the effectiveness of the STORM system. The primary dataset used in the study is the FreshWiki dataset, which is created by curating recent high-quality Wikipedia articles to avoid data leakage during pretraining.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "The study of STORM uses two datasets: FreshWiki and a dataset of recent high-quality Wikipedia articles. FreshWiki is a dataset curated by the researchers, which contains recent, high-quality Wikipedia articles. The dataset of Wikipedia articles is used to evaluate the performance of STORM. The articles in this dataset are selected based on certain criteria, such as having fewer than 3000 words and being of high quality. The researchers also use a dataset of 100 samples from FreshWiki to compare the performance of STORM with other baselines. Additionally, the study uses a dataset of 20 topics to evaluate the articles generated by STORM and the oRAG baseline. \n",
      "\n",
      "The FreshWiki dataset is created by curating recent, high-quality Wikipedia articles to avoid data leakage during pretraining. The dataset is used to evaluate the pre-writing stage of STORM and to establish evaluation criteria for both outline and final article quality. The dataset of Wikipedia articles is obtained from Wikipedia, and the researchers select articles that meet certain criteria, such as having fewer than 3000 words and being of high quality. \n",
      "\n",
      "The study also uses other datasets, such as the dataset used in the experiments, which has an average number of sections of 8.4, an average number of all-level headings of 15.8, an average length of a section of 327.8, an average length of the total article of 2159.1, and an average number of references of 90.1. \n",
      "\n",
      "Overall, the study uses a combination of curated and existing datasets to evaluate the performance of STORM and to establish evaluation criteria for the pre-writing stage of long-form article generation.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"What are dataset used in the study and how they are created?\"\n",
    "for m in model_list:\n",
    "    answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0bce3bf-b52e-4497-ab84-df79e6f44520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study of STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, utilizes the following datasets:\n",
      "\n",
      "1.  FreshWiki: A dataset of recent high-quality Wikipedia articles, curated to avoid data leakage during pretraining. This dataset is used to evaluate the effectiveness of STORM in generating grounded long-form articles.\n",
      "2.  Wikipedia articles: The study also uses a set of recent Wikipedia articles to evaluate the performance of STORM in generating long-form articles.\n",
      "\n",
      "The FreshWiki dataset is created by selecting 100 samples from the dataset with human-written articles not exceeding 3000 words. These samples are then used to train and evaluate the STORM system.\n",
      "\n",
      "The study also mentions that the Wikipedia articles used in the dataset are curated to avoid data leakage during pretraining. This means that the dataset is not used to train the STORM system, but rather to evaluate its performance on a separate dataset.\n",
      "\n",
      "The FreshWiki dataset is a valuable resource for the study of STORM, as it provides a large and diverse set of Wikipedia articles that can be used to evaluate the effectiveness of the writing system.\n",
      "CPU times: user 810 ms, sys: 35.9 ms, total: 846 ms\n",
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m = \"us.meta.llama3-2-1b-instruct-v1:0\"\n",
    "original_message = \"What are dataset used in the study and how they are created?\"\n",
    "answer, rr_c = batch_conversation(original_message=original_message, model_name=m)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44c0c445-7d36-4bbb-891d-6b7ead1eaad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rr_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c4c981e-a441-4f37-b493-40fc7c3ac193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section': 'Abstract', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 1}\n",
      "{'section': '1 Introduction', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 3}\n",
      "{'section': '5 Results and Analysis', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 20}\n",
      "{'section': '8 Conclusion', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 27}\n",
      "{'section': '3.1 Perspective-Guided Question Asking', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 9}\n",
      "{'section': '4 Experiments', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 13}\n",
      "{'section': '3 Method', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 8}\n",
      "{'section': '3.2 Simulating Conversations', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 10}\n",
      "{'section': 'Algorithm 1: STORM', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 46}\n",
      "{'section': '3.3 Creating the Article Outline', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 11}\n",
      "{'section': 'B Pseudo Code of STORM', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 39}\n",
      "{'section': 'C Automatic Evaluation Details', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 42}\n",
      "{'section': '6 Human Evaluation', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 24}\n",
      "{'section': '6 Human Evaluation', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 23}\n",
      "{'section': '4.4 STORM Implementation', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 19}\n",
      "{'section': 'References', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 37}\n",
      "{'section': '4.3 Baselines', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 15}\n",
      "{'section': '2.2 Outline Creation and Evaluation', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 7}\n",
      "{'section': '4.4 STORM Implementation', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 16}\n",
      "{'section': '7 Related Works', 'source': '.docs/test1/storm.md', 'type': 'document', 'sequence': 26}\n"
     ]
    }
   ],
   "source": [
    "for c in rr_c:\n",
    "    print(c.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837eb077-a952-4e3d-93f9-60da677104ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai-ra",
   "language": "python",
   "name": "broai-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
