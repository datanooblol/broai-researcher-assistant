{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d61a8a4-6a32-4254-80c5-de4388f227a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cd0c3-0ab1-42c1-9644-1f670184673e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04460b5-436c-42de-947f-6479c9200be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"./memories.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7bfb94-451a-4ef8-a6e9-ca69453770c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.prompt_management.interface import Persona, Instructions, Examples, Example\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from broai.experiments.bro_agent import BroAgent\n",
    "import json\n",
    "from broai.interface import Context, Contexts\n",
    "from broai.experiments.vector_store import DuckVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90163eda-1ed4-4265-9183-c93a957ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.jargon_store import JargonStore, JargonRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea043c71-d617-4ab9-a351-69d1043f2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23728/2186289470.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb15de4-b7c4-4a32-8c33-57d4a5588d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23728/2466456318.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 190650.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension\n",
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ee5a28-a41d-4cb6-86db-f3c979438387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23728/1440006260.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_23728/1440006260.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
      "/tmp/ipykernel_23728/1440006260.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n"
     ]
    }
   ],
   "source": [
    "raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
    "enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
    "longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n",
    "jargon_memory = JargonStore(db_name=DB_NAME, table=\"jargon_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce04192-42f7-44c5-a0f7-b09280c5f262",
   "metadata": {},
   "source": [
    "# Agent Flows: \n",
    "- JargonDetector\n",
    "- JargonEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a806cf1-49d3-4483-a24e-515d0d28da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.jargon_detector import JargonDetector, InputMessage\n",
    "from agents.jargon_editor import JargonEditor, InputEditMessage\n",
    "from agents.query_decomposer import QueryDecomposer, InputMessage\n",
    "from agents.oracle import Oracle, InputOracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d545c43f-7d0c-47da-be81-c817a45ad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_conversation(original_message, model_name=\"us.meta.llama3-2-11b-instruct-v1:0\"):\n",
    "    potential_jargons = JargonDetector.run(request=InputMessage(message=original_message))\n",
    "    detected_jargons = [j for j in potential_jargons.jargons if j.confidence>.5]\n",
    "    proxy_message = original_message\n",
    "    if len(detected_jargons) > 0:\n",
    "        jargon_knowledges = []\n",
    "        for j in detected_jargons:\n",
    "            jk = jargon_memory.fulltext_search(search_query=\"STORM\")\n",
    "            jargon_knowledges.extend(jk)\n",
    "    \n",
    "        jargon_knowledges_str = \"\\n\\n\".join([f\"{enum+1}: {j.jargon}\\nEvidence: {j.evidence}\\nExplanation: {j.explanation}\" for enum, j in enumerate(jargon_knowledges)])\n",
    "        edited_message = JargonEditor.run(InputEditMessage(knowledge=jargon_knowledges_str, message=original_message))\n",
    "        proxy_message = edited_message.edited_message\n",
    "        \n",
    "    sub_queries = QueryDecomposer.run(InputMessage(message=proxy_message))\n",
    "    retreived_contexts = []\n",
    "    for sq in sub_queries.sub_queries:\n",
    "        rc = longterm_memory.vector_search(search_query=sq, limit=10)\n",
    "        retreived_contexts.extend(rc)\n",
    "    id_list = []\n",
    "    deduplicated_contexts = []\n",
    "    for c in retreived_contexts:\n",
    "        if c.id not in id_list:\n",
    "            id_list.append(c.id)\n",
    "            deduplicated_contexts.append(c)\n",
    "    reranked_contexts, scores = rr.run(search_query=proxy_message, contexts=deduplicated_contexts, top_n=10)\n",
    "    prior_knowledge = \"\\n\\n\".join([f\"{c.context}\" for c in reranked_contexts])\n",
    "    Oracle.model.model_name = model_name\n",
    "    # answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "    answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=\", \".join(sub_queries.sub_queries)))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495c4c54-1818-40ac-aee0-baa17b68f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PotentialJargon(jargon='STORM', confidence=0.8)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original_message = \"What does STORM do in the research study?\"\n",
    "original_message = \"Explain how STORM works in the study to me like I'm a five years old.\"\n",
    "potential_jargons = JargonDetector.run(request=InputMessage(message=original_message))\n",
    "detected_jargons = [j for j in potential_jargons.jargons if j.confidence>.5]\n",
    "detected_jargons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a8f498c-6d85-45d5-b523-9dd4cc76599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargon_knowledges = []\n",
    "for j in detected_jargons:\n",
    "    jk = jargon_memory.fulltext_search(search_query=\"STORM\")\n",
    "    jargon_knowledges.extend(jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd089142-3db7-48e4-ba38-55bbba916b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "jargon_knowledges_str = \"\\n\\n\".join([f\"{enum+1}: {j.jargon}\\nEvidence: {j.evidence}\\nExplanation: {j.explanation}\" for enum, j in enumerate(jargon_knowledges)])\n",
    "print(jargon_knowledges_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2cfa7d2-d33c-41be-a1fd-a4d442a27ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_message = JargonEditor.run(InputEditMessage(knowledge=jargon_knowledges_str, message=original_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2c70d0c-249b-4272-8196-6a17b914f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain how STORM works in the study to me like I'm a five years old.\n",
      "==========\n",
      "Explain how STORM (Stochastic Optical Reconstruction Microscopy), a type of super-resolution microscopy, works in the study in a simple way, like I'm a five years old.\n"
     ]
    }
   ],
   "source": [
    "print(original_message)\n",
    "print(\"=\"*10)\n",
    "print(edited_message.edited_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2362d58-ee28-4668-b300-751c79df3d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecomposedQueries(sub_queries=['What does STORM do in the research study', 'STORM in the research study'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryDecomposer.run(InputMessage(message=original_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56228a3-7501-4a41-9f57-5ef82d274d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What does STORM do',\n",
       " 'STORM system',\n",
       " 'Automated pre-writing stage',\n",
       " 'Simulates conversations',\n",
       " 'Discovers different perspectives',\n",
       " 'Writing system for Synthesis of Topic Outlines',\n",
       " 'Retrieval and Multi-perspective Question Asking']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_queries = QueryDecomposer.run(InputMessage(message=edited_message.edited_message))\n",
    "sub_queries.sub_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e92889-96d6-4595-8a33-ef981676141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_contexts = []\n",
    "for sq in sub_queries.sub_queries:\n",
    "    rc = longterm_memory.vector_search(search_query=sq, limit=10)\n",
    "    retreived_contexts.extend(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d766e7b-fe0c-444e-b1c1-126e2907ff9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retreived_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0caf2fc6-3959-4381-ae9e-dedbebd26927",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "deduplicated_contexts = []\n",
    "for c in retreived_contexts:\n",
    "    if c.id not in id_list:\n",
    "        id_list.append(c.id)\n",
    "        deduplicated_contexts.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13a80cf0-2343-405b-bf09-c0d0d08c48dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retreived_contexts), len(deduplicated_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9bef55f-1257-4d02-9788-ec58fdc93463",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_knowledge = \"\\n\\n\".join([f\"{c.context}\" for c in deduplicated_contexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2acd700-513e-4a5e-bc20-88e3b0522f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM is a system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. It discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. STORM prompts an LLM to generate a list of related topics and subsequently extracts the tables of contents from their corresponding Wikipedia articles, if such articles can be obtained through Wikipedia API. These tables of contents are concatenated to create a context to prompt the LLM to identify N perspectives P = {p1, ..., p<sup>N</sup> } that can collectively contribute to a comprehensive article on t. STORM creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-2-3b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b8cfe34-51d0-493c-9b8a-9ccb1551f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM simulates conversations between a Wikipedia writer and a topic expert, discovers different perspectives by surveying articles, and uses these perspectives to control question asking, prompting an LLM to generate a list of related topics and extract tables of contents to identify N perspectives that contribute to a comprehensive article.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-2-11b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad216660-2900-4b08-b747-075520367fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM simulates conversations between a Wikipedia writer and a topic expert, discovers different perspectives by surveying articles, and creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f114621-10b5-4f48-b370-45e75dc281e1",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a47af6-b519-4f3e-9e22-aaaf4b766d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    \"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "    \"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1000e7fc-4266-423d-afe7-04b62e4cdd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. STORM simulates a conversation between a Wikipedia writer and a topic expert to generate questions and answers, using LLM and trusted sources to ensure factual information.\n",
      "\n",
      "The system consists of three main stages:\n",
      "\n",
      "1. **Discovering different perspectives**: STORM discovers different perspectives by surveying existing articles from similar topics and extracting tables of contents from their corresponding Wikipedia articles. These tables of contents are concatenated to create a context to prompt the LLM to identify N perspectives that can collectively contribute to a comprehensive article on the given topic.\n",
      "\n",
      "2. **Simulating conversations**: STORM simulates a conversation between a Wikipedia writer and a topic expert. In the i-th round of the conversation, the LLM-powered Wikipedia writer generates a single question q<sup>i</sup> based on the topic t, its assigned perspective p ∈ P, and the conversation history {q1, a1, ..., qi−1, ai−1} where a<sup>j</sup> denotes the simulated expert's answer. The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions.\n",
      "\n",
      "3. **Creating the article outline**: After thoroughly researching the topic through N + 1 simulated conversations, STORM creates an outline before the actual writing starts. It first prompts the model to generate a draft outline O<sup>D</sup> given only the topic t. Then, the LLM is prompted with the topic t, the draft outline OD, and the simulated conversations {C0, C1, ..., C<sup>N</sup> } to refine the outline, resulting in an improved outline O which will be used for producing the full-length article.\n",
      "\n",
      "STORM uses a novel multi-stage approach to automate the pre-writing stage, leveraging the capabilities of LLMs to conduct research, ask incisive questions, and retrieve trusted information from the Internet. The system has been evaluated using a dataset of recent Wikipedia articles and has demonstrated its effectiveness in generating grounded and organized long-form articles.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. \n",
      "\n",
      "STORM discovers different perspectives by surveying articles and uses them to control question asking, prompting an LLM to generate a list of related topics and extract tables of contents to identify N perspectives that contribute to a comprehensive article.\n",
      "\n",
      "STORM simulates a conversation between a Wikipedia writer and a topic expert to generate questions and answers, using LLM and trusted sources to ensure factual information.\n",
      "\n",
      "STORM creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge.\n",
      "\n",
      "The STORM system is designed to assist the creation of grounded, long-form articles, without depending extensively on prompt engineering for a single domain. It is a general framework that can be used to generate articles on various topics.\n",
      "\n",
      "STORM's main components include:\n",
      "\n",
      "- Discovering different perspectives by surveying articles\n",
      "- Simulating conversations between a writer and an expert\n",
      "- Creating an outline for an article\n",
      "- Retrieval and multi-perspective question asking\n",
      "\n",
      "STORM's algorithm is as follows:\n",
      "\n",
      "1. Input: Topic t, maximum perspective N, maximum conversation round M\n",
      "2. Output: Outline O, references R\n",
      "3. Discover perspectives P\n",
      "4. Simulate conversations\n",
      "5. Create the outline\n",
      "6. Return O, R\n",
      "\n",
      "STORM has been evaluated using the FreshWiki dataset and has shown to produce better results than a baseline system, especially in terms of outline quality and breadth of coverage.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating long-form articles, such as Wikipedia-like articles. It discovers different perspectives on a given topic, simulates conversations between a writer and an expert, and creates a comprehensive outline. The system uses large language models (LLMs) to ask incisive questions, retrieve trusted information from the Internet, and generate an outline that can be expanded into a full-length article. STORM's key features include:\n",
      "\n",
      "1. **Discovering different perspectives**: STORM surveys existing articles from similar topics and uses these perspectives to control the question-asking process.\n",
      "2. **Simulating conversations**: The system simulates a conversation between a Wikipedia writer and a topic expert, using LLMs to generate questions and answers based on the topic, perspective, and conversation history.\n",
      "3. **Creating an outline**: STORM generates a draft outline from the topic and refines it with simulated conversations and LLM knowledge.\n",
      "4. **Retrieval and multi-perspective question asking**: The system uses trusted sources from the Internet to ground answers to questions and ensures that the conversation history provides factual information.\n",
      "\n",
      "By automating the pre-writing stage, STORM aims to facilitate individuals in initiating in-depth learning about a topic and reduce the time and effort required for expository writing. The system has been evaluated using a dataset of recent, high-quality Wikipedia articles and has been found to outperform an outline-driven retrieval-augmented baseline in terms of article breadth and organization.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"What does STORM do in the research study?\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45fc33ba-b09e-4000-805d-e14cf12f69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "The study used the FreshWiki dataset, which consists of 100 samples of human-written articles under 3000 words. This dataset was selected for comparison with the proposed STORM system.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "The dataset used in the STORM study is called FreshWiki, which is a dataset of recent high-quality Wikipedia articles.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "The dataset used in the STORM study is called FreshWiki, which is a collection of recent high-quality Wikipedia articles. This dataset was curated for the purpose of evaluating the effectiveness of the STORM system in generating outlines and full-length articles.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"What does the dataset used in the study?\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "359be844-d9ed-471c-bd22-b73da45edf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "The STORM writing system is a framework that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It works by discovering different perspectives on a given topic, simulating conversations between a Wikipedia writer and a topic expert, and creating a comprehensive outline.\n",
      "\n",
      "Here's a step-by-step explanation of how STORM works:\n",
      "\n",
      "1. **Discovering perspectives**: STORM identifies different perspectives on a given topic by surveying existing articles from similar topics. It uses these perspectives to control the question asking process.\n",
      "\n",
      "2. **Simulating conversations**: STORM simulates a conversation between a Wikipedia writer and a topic expert. In each round of the conversation, the LLM-powered Wikipedia writer generates a single question based on the topic, its assigned perspective, and the conversation history. The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions.\n",
      "\n",
      "3. **Question answering**: The LLM synthesizes trustworthy sources to generate answers to the questions. These sources are added to the reference list R for full article generation.\n",
      "\n",
      "4. **Creating the outline**: After thoroughly researching the topic through N + 1 simulated conversations, STORM creates an outline by generating a draft outline from the topic and refining it with the simulated conversations and LLM knowledge.\n",
      "\n",
      "5. **Refining the outline**: The LLM refines the outline by incorporating the simulated conversations and its internal knowledge.\n",
      "\n",
      "The STORM system consists of the following components:\n",
      "\n",
      "- **Perspective discovery**: This component identifies different perspectives on a given topic by surveying existing articles from similar topics.\n",
      "- **Conversation simulation**: This component simulates a conversation between a Wikipedia writer and a topic expert.\n",
      "- **Question asking**: This component generates questions based on the topic, its assigned perspective, and the conversation history.\n",
      "- **Question answering**: This component synthesizes trustworthy sources to generate answers to the questions.\n",
      "- **Outline creation**: This component generates a draft outline from the topic and refines it with the simulated conversations and LLM knowledge.\n",
      "\n",
      "The STORM system is evaluated using a dataset of recent Wikipedia articles, and the results show that it outperforms two baseline models in terms of heading soft recall, entity recall, and full-length article quality. The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating long-form articles from scratch. It is designed to generate articles with comparable breadth and depth to Wikipedia pages. The system works by discovering different perspectives on a given topic, simulating conversations between a writer and an expert, and creating an outline for the article.\n",
      "\n",
      "Here's a step-by-step explanation of how STORM works:\n",
      "\n",
      "1. **Discovering perspectives**: STORM surveys existing articles from similar topics to identify various perspectives on covering the topic. It uses these perspectives to control the question-asking process.\n",
      "2. **Simulating conversations**: STORM simulates conversations between a Wikipedia writer and a topic expert. The writer generates questions based on the topic, its assigned perspective, and the conversation history. The expert's answers are grounded on trusted Internet sources.\n",
      "3. **Creating an outline**: STORM creates an outline for the article by generating a draft outline from the topic and refining it with simulated conversations and LLM knowledge.\n",
      "\n",
      "STORM's writing system is designed to assist the creation of grounded, long-form articles without depending extensively on prompt engineering for a single domain. The system uses a framework that automates the pre-writing stage by discovering perspectives, simulating conversations, and creating outlines for long-form articles.\n",
      "\n",
      "The study evaluates the performance of STORM by comparing it with two variants: STORM w/o Perspective and STORM w/o Conversation. The results show that the full pipeline produces the best results, with removing the outline stage significantly deteriorating performance.\n",
      "\n",
      "STORM's effectiveness is demonstrated through its ability to generate outlines with high recall, entity recall, and full-length article quality. The system also shows promise in improving the quality of articles generated by other writing systems, such as RAG and oRAG.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating long-form articles, such as Wikipedia pages. It works by discovering different perspectives on a given topic, simulating conversations between a writer and an expert, and creating an outline based on the collected information. \n",
      "\n",
      "The process starts with discovering perspectives by surveying existing articles on similar topics and using these perspectives to control the question-asking process. STORM then simulates conversations between a Wikipedia writer and a topic expert, where the writer asks questions based on the topic and the assigned perspective, and the expert provides answers grounded on trustworthy online sources.\n",
      "\n",
      "The conversation history is used to update the understanding of the topic and ask follow-up questions. The answers to the questions are used to create a draft outline, which is then refined using the simulated conversations and the internal knowledge of the large language model (LLM).\n",
      "\n",
      "The refined outline is used to generate a full-length article. The study evaluates the performance of STORM by comparing it with two variants: STORM without perspective and STORM without conversation. The results show that the full STORM pipeline produces the best results, with removing the outline stage significantly deteriorating performance.\n",
      "\n",
      "STORM's key components include:\n",
      "\n",
      "1. **Perspective discovery**: STORM discovers different perspectives on a topic by surveying existing articles and using these perspectives to control the question-asking process.\n",
      "2. **Simulated conversations**: STORM simulates conversations between a writer and an expert, where the writer asks questions based on the topic and the assigned perspective, and the expert provides answers grounded on trustworthy online sources.\n",
      "3. **Outline creation**: STORM creates a draft outline based on the conversation history and refines it using the simulated conversations and the internal knowledge of the LLM.\n",
      "4. **Full-length article generation**: STORM uses the refined outline to generate a full-length article.\n",
      "\n",
      "Overall, STORM is a framework that automates the pre-writing stage by discovering perspectives, simulating conversations, and creating outlines for long-form articles, and has been shown to produce high-quality articles with improved breadth and depth.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"How does STORM work in the study?\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d6deaa0-3903-454c-8500-a1241f68ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "Based on the provided prior knowledge, here's a summary of how STORM works:\n",
      "\n",
      "STORM is a writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It consists of three main stages:\n",
      "\n",
      "1. **Discovering perspectives**: STORM discovers different perspectives on a given topic by surveying existing Wikipedia articles from similar topics. It uses these perspectives to control the question asking process.\n",
      "2. **Simulating conversations**: STORM simulates a conversation between a Wikipedia writer and a topic expert. The writer generates questions based on the topic, its assigned perspective, and the conversation history. The expert's answers are grounded on trusted sources from the Internet.\n",
      "3. **Creating the outline**: STORM creates an outline by generating a draft outline from the topic and refining it with simulated conversations and LLM knowledge.\n",
      "\n",
      "The algorithm for STORM is as follows:\n",
      "\n",
      "1. Discover perspectives P by generating related topics, extracting tables of contents from corresponding Wikipedia articles, and concatenating them to create a context.\n",
      "2. Simulate conversations by generating questions, answering them, and updating the conversation history.\n",
      "3. Create the outline by generating a draft outline from the topic and refining it with simulated conversations and LLM knowledge.\n",
      "\n",
      "STORM's pre-writing stage automation aims to improve the quality of Wikipedia-like articles by considering multiple perspectives and simulated conversations. The system has been evaluated using the FreshWiki dataset and has shown promising results, with expert feedback highlighting the need to address challenges such as source bias transfer and over-association of unrelated facts.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It works by discovering different perspectives on a given topic, simulating conversations between a Wikipedia writer and a topic expert, and creating an outline for the article.\n",
      "\n",
      "STORM's pre-writing stage automation involves the following steps:\n",
      "\n",
      "1. Discovering perspectives: STORM surveys existing articles from similar topics to identify different perspectives on the given topic.\n",
      "2. Simulating conversations: STORM simulates a conversation between a Wikipedia writer and a topic expert, using large language models (LLMs) to generate questions and answers.\n",
      "3. Creating an outline: STORM creates an outline for the article based on the perspectives and conversations generated in the previous steps.\n",
      "\n",
      "The simulated conversations in STORM involve the following steps:\n",
      "\n",
      "1. Question asking: The LLM-powered Wikipedia writer generates a single question based on the topic, its assigned perspective, and the conversation history.\n",
      "2. Question answering: The conversation history is used to update the LLM's understanding of the topic, and the LLM generates an answer to the question.\n",
      "3. Answer grounding: The answer is grounded on trusted sources from the Internet to ensure factual information.\n",
      "\n",
      "STORM's discovery of perspectives involves the following steps:\n",
      "\n",
      "1. Generating related topics: STORM generates a list of related topics based on the given topic.\n",
      "2. Extracting tables of contents: STORM extracts the tables of contents from the related topics' Wikipedia articles.\n",
      "3. Identifying perspectives: STORM identifies different perspectives by analyzing the tables of contents and generating a list of related topics.\n",
      "\n",
      "Overall, STORM's pre-writing stage automation enables the creation of well-organized and logically structured articles, with a high level of coherence and engagement.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It works by discovering diverse perspectives on a given topic, simulating conversations between a Wikipedia writer and a topic expert, and creating an outline based on the collected information. \n",
      "\n",
      "The process starts with discovering perspectives by surveying existing articles from similar topics and using these perspectives to control the question-asking process. STORM then simulates a conversation between a Wikipedia writer and a topic expert, where the writer generates questions based on the topic and its assigned perspective, and the expert provides answers grounded on trusted sources from the Internet.\n",
      "\n",
      "The conversation history enables the writer to update its understanding of the topic and ask follow-up questions. The trustworthy sources used to answer the questions are added to a set of references for full article generation. Finally, STORM creates an outline by prompting a large language model to generate a draft outline given the topic and then refining it with the simulated conversations and the model's internal knowledge.\n",
      "\n",
      "The pre-writing stage automation in STORM involves researching the topic, creating an outline, and simulating conversations to generate a full-length article. The system is designed to assist in the creation of grounded, long-form articles without depending extensively on prompt engineering for a single domain. \n",
      "\n",
      "STORM's approach has been evaluated using a dataset of recent, high-quality Wikipedia articles, and the results show that it outperforms an outline-driven retrieval-augmented baseline in terms of article organization and breadth of coverage. Expert feedback from experienced Wikipedia editors also highlights the effectiveness of STORM in generating high-quality articles and identifies new challenges for future research, such as addressing source bias transfer and over-association of unrelated facts.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study.\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd3fff9-fd86-4644-a894-2e92d9c57bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "Here's a simplified explanation of how STORM works:\n",
      "\n",
      "**What is STORM?**\n",
      "STORM is a system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. It uses large language models (LLMs) to research a topic, create an outline, and simulate conversations between a writer and a topic expert.\n",
      "\n",
      "**How does STORM work?**\n",
      "\n",
      "1. **Discovering perspectives**: STORM starts by surveying existing articles from similar topics to discover different perspectives on the given topic.\n",
      "2. **Simulating conversations**: STORM simulates a conversation between a Wikipedia writer and a topic expert. The writer asks questions based on the topic and its assigned perspective, and the expert provides answers grounded on trustworthy online sources.\n",
      "3. **Creating an outline**: After the conversations, STORM creates an outline by generating a draft outline from the topic and refining it with the simulated conversations and LLM knowledge.\n",
      "4. **Refining the outline**: The outline is further refined by adding more information and organizing it in a logical structure.\n",
      "\n",
      "**Key components of STORM**\n",
      "\n",
      "* **Perspective-guided question asking**: STORM uses diverse perspectives to guide question asking, which helps to elicit more in-depth questions and gather more information.\n",
      "* **Simulated conversations**: STORM simulates conversations between a writer and a topic expert to gather more information and create a comprehensive outline.\n",
      "* **LLM knowledge**: STORM uses LLMs to provide answers to questions and to generate the outline.\n",
      "\n",
      "**Benefits of STORM**\n",
      "\n",
      "* **Automates the pre-writing stage**: STORM automates the pre-writing stage, which is a challenging task for humans.\n",
      "* **Improves outline and article quality**: STORM's simulation of conversations and use of LLM knowledge help to create a comprehensive outline and improve article quality.\n",
      "* **Helps surface new challenges**: STORM's expert evaluation reveals new challenges for grounded writing systems, such as source bias transfer and over-association of unrelated facts.\n",
      "\n",
      "Overall, STORM is a system that uses LLMs to automate the pre-writing stage for creating Wikipedia-like articles from scratch. It simulates conversations between a writer and a topic expert, uses diverse perspectives to guide question asking, and creates a comprehensive outline using LLM knowledge.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that helps generate long-form articles from scratch, similar to Wikipedia pages. Here's how it works in plain English:\n",
      "\n",
      "1. **Discovering perspectives**: STORM looks at existing articles on similar topics to find different perspectives on the topic. This helps it ask more in-depth questions and gather a variety of information.\n",
      "2. **Simulating conversations**: STORM simulates a conversation between a Wikipedia writer and a topic expert. The writer asks questions, and the expert provides answers based on trusted sources from the Internet.\n",
      "3. **Generating an outline**: STORM uses the information gathered from the conversations to create an outline for the article. This outline is then refined to ensure it's well-organized and logically structured.\n",
      "\n",
      "STORM's ability to automate the pre-writing stage is a key feature. This stage is crucial in writing long-form articles, as it requires thorough research and planning. STORM's system helps with this process by:\n",
      "\n",
      "* Asking effective questions to research the topic\n",
      "* Gathering a variety of information from trusted sources\n",
      "* Creating a well-organized and logically structured outline\n",
      "\n",
      "STORM's simulation of conversations between a Wikipedia writer and a topic expert is another important aspect. This simulation helps ensure that the information gathered is accurate and trustworthy. The expert's answers are based on reliable sources, which helps prevent the spread of misinformation.\n",
      "\n",
      "STORM's ability to discover different perspectives by surveying existing articles from similar topics is also a significant feature. This helps the system ask more in-depth questions and gather a variety of information, which is essential for creating well-rounded and informative articles.\n",
      "\n",
      "Overall, STORM is a powerful tool for generating long-form articles from scratch. Its ability to automate the pre-writing stage, simulate conversations, and discover different perspectives makes it an effective system for creating high-quality articles.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage for creating long-form articles, similar to those found on Wikipedia. In plain English, STORM works by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article.\n",
      "\n",
      "STORM's ability to automate the pre-writing stage is based on its capacity to discover different perspectives by surveying existing articles from similar topics. This allows STORM to create a comprehensive outline that covers various aspects of the topic. STORM achieves this by prompting a large language model (LLM) to generate a list of related topics and extract tables of contents from their corresponding Wikipedia articles.\n",
      "\n",
      "STORM simulates conversations between a Wikipedia writer and a topic expert to generate questions and answers. The writer, powered by an LLM, asks questions based on the topic, its assigned perspective, and the conversation history. The expert's answers are grounded on trusted sources from the Internet, which are evaluated using a rule-based filter to exclude untrustworthy sources.\n",
      "\n",
      "The simulation of conversations enables STORM to ask follow-up questions, update its understanding of the topic, and provide factual information. This process allows STORM to create a refined outline that can be used to generate a full-length article.\n",
      "\n",
      "STORM's ability to discover different perspectives is a key aspect of its functionality. By surveying existing articles from similar topics, STORM can identify various perspectives that contribute to a comprehensive article. This is achieved by prompting an LLM to generate a list of related topics and extract tables of contents from their corresponding Wikipedia articles. The perspectives are then used to guide the question-asking process, ensuring that the conversation covers a wide range of aspects related to the topic.\n",
      "\n",
      "Overall, STORM's automation of the pre-writing stage, simulation of conversations, and discovery of different perspectives make it a powerful tool for generating high-quality, long-form articles.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study in plain English.\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb8154a-591a-4401-8559-ef4ea05a3c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "STORM is a system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. Here's how it works:\n",
      "\n",
      "1. **Researching the topic**: STORM starts by researching the given topic via effective question asking. This involves generating a list of related topics and extracting tables of contents from corresponding Wikipedia articles.\n",
      "2. **Discovering perspectives**: The system discovers different perspectives by surveying existing articles from similar topics. These perspectives are used to control the question asking process.\n",
      "3. **Simulating conversations**: STORM simulates multi-turn conversations between a writer and an expert grounded on trusted online sources. This involves generating questions, answering them, and refining the outline based on the collected information.\n",
      "4. **Creating the outline**: After researching the topic and simulating conversations, STORM creates an outline by generating a draft outline from the topic and refining it with the simulated conversations and LLM knowledge.\n",
      "5. **Generating the article**: The final step is to generate the full-length article using the refined outline.\n",
      "\n",
      "STORM uses a multi-stage approach to automate the pre-writing stage. It first discovers diverse perspectives by retrieving and analyzing Wikipedia articles from similar topics. Then, it simulates conversations between a Wikipedia writer and an expert grounded on trustworthy online sources. The final outline is curated based on the LLM's intrinsic knowledge and the gathered conversations from different perspectives.\n",
      "\n",
      "The system is evaluated using a dataset of recent Wikipedia articles, and the results show that STORM outperforms a retrieval-augmented baseline in terms of heading soft recall, entity recall, and full-length article quality. Human evaluation and citation quality analysis also support the effectiveness of STORM.\n",
      "\n",
      "In simple terms, STORM is like a research assistant that helps you write a long article by:\n",
      "\n",
      "* Asking good questions to find relevant information\n",
      "* Talking to experts to get their opinions\n",
      "* Organizing the information into a clear structure\n",
      "* Writing the article based on the outline\n",
      "\n",
      "Imagine you want to write an article about a topic you're not familiar with. STORM would help you by:\n",
      "\n",
      "* Asking questions to find relevant information\n",
      "* Talking to experts to get their opinions\n",
      "* Organizing the information into a clear structure\n",
      "* Writing the article based on the outline\n",
      "\n",
      "This way, you can write a well-researched and well-organized article without having to do all the research and planning yourself.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "Based on the provided prior knowledge, here are the answers to the questions:\n",
      "\n",
      "1. How STORM automates the pre-writing stage:\n",
      "\n",
      "STORM automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. It uses a multi-stage approach to discover diverse perspectives, simulate conversations, and create an outline.\n",
      "\n",
      "2. How STORM simulates conversations between a writer and an expert:\n",
      "\n",
      "STORM simulates conversations between a writer and an expert by using a large language model (LLM) to ask incisive questions and retrieve trusted information from the Internet. The LLM is prompted to generate a set of questions, and then it simulates a conversation with an expert to gather more information.\n",
      "\n",
      "3. How STORM discovers different perspectives:\n",
      "\n",
      "STORM discovers different perspectives by surveying existing articles from similar topics and using these perspectives to control the question asking process. It prompts an LLM to generate a list of related topics and extracts the tables of contents from their corresponding Wikipedia articles.\n",
      "\n",
      "4. How STORM controls the question asking process:\n",
      "\n",
      "STORM controls the question asking process by using the discovered perspectives to guide the LLM in asking effective questions. It prompts the LLM to generate a set of questions, and then it simulates a conversation with an expert to gather more information.\n",
      "\n",
      "5. Explain STORM in simple terms for a five-year-old:\n",
      "\n",
      "Imagine you want to write a big story about a topic, like a cat. STORM is like a helper that asks you questions to make sure you have all the right information. It looks at other people's stories about cats and finds out what they say about them. Then, it talks to a special computer that knows a lot of things, and it asks the computer questions to get even more information. Finally, it helps you make an outline of what you want to say, so you can write your story.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "I'll break down the information into smaller parts to explain how STORM works and answer your questions.\n",
      "\n",
      "**How STORM automates the pre-writing stage:**\n",
      "STORM is a system that helps write articles from scratch. It automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert. This process helps generate a full-length article.\n",
      "\n",
      "**How STORM simulates conversations between a writer and an expert:**\n",
      "STORM simulates conversations by asking questions about a topic, just like a writer would ask an expert. It uses the answers to these questions to create an outline and eventually a full article. The conversations are simulated using a large language model that can understand and respond to questions.\n",
      "\n",
      "**How STORM discovers different perspectives:**\n",
      "STORM discovers different perspectives by surveying existing articles on similar topics. It uses these perspectives to control the question-asking process, which helps create a more comprehensive article.\n",
      "\n",
      "**How STORM controls the question-asking process:**\n",
      "STORM controls the question-asking process by using the discovered perspectives to guide the questions. It prompts a large language model to ask questions about a topic from different perspectives, which helps create a more detailed and organized article.\n",
      "\n",
      "**Explain STORM in simple terms for a five-year-old:**\n",
      "Imagine you want to write a story about animals. STORM is like a magic tool that helps you write the story. It first asks questions about animals, like \"What do animals eat?\" or \"Where do animals live?\" Then, it uses the answers to create a plan for the story, like a list of what to write about. Finally, it helps write the story using the plan. It's like having a helper that makes writing easier and more fun!\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study to me like I'm a five years old.\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7b9af7c-d5a5-4454-ab6e-c9e16584766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "Based on the provided prior knowledge, STORM is a system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. \n",
      "\n",
      "Here's a summary of how STORM works:\n",
      "\n",
      "1. **Perspective-Guided Question Asking**: STORM discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. It prompts an LLM to generate a list of related topics and subsequently extracts the tables of contents from their corresponding Wikipedia articles. These tables of contents are concatenated to create a context to prompt the LLM to identify N perspectives that can collectively contribute to a comprehensive article on the given topic.\n",
      "\n",
      "2. **Simulated Conversations**: STORM simulates multi-turn conversations between a Wikipedia writer and a topic expert grounded on trusted Internet sources. The writer asks questions guided by the discovered perspectives, and the expert responds with answers. This process is repeated for a maximum of M conversation rounds.\n",
      "\n",
      "3. **Outline Creation**: After the simulated conversations, STORM creates an outline by refining the initial draft outline generated by the LLM. The refined outline is then used to produce the full-length article.\n",
      "\n",
      "4. **Reference Collection**: STORM also collects references by searching and sifting the queries generated during the conversations. These references are added to the outline and used to generate the full-length article.\n",
      "\n",
      "The study evaluates STORM's effectiveness in generating grounded, long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. The results show that STORM outperforms a baseline approach that relies on retrieval-augmented generation and improves the organization and breadth of the generated articles.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. Here's how STORM works:\n",
      "\n",
      "1. **Perspective discovery**: STORM discovers different perspectives by surveying existing articles from similar topics. It uses these perspectives to control the question asking process.\n",
      "2. **Question asking**: STORM prompts a language model to generate a list of related topics and extracts tables of contents from their corresponding Wikipedia articles. These tables of contents are concatenated to create a context to prompt the language model to identify N perspectives that can collectively contribute to a comprehensive article.\n",
      "3. **Simulated conversations**: STORM simulates conversations between a Wikipedia writer and a topic expert. The writer asks questions guided by the perspectives, and the expert provides answers grounded on trusted Internet sources.\n",
      "4. **Outline creation**: STORM creates an outline by refining the draft outline generated by the language model using the simulated conversations and the language model's internal knowledge.\n",
      "5. **Full-length article generation**: STORM uses the outline to generate a full-length article.\n",
      "\n",
      "STORM's conversation simulation involves a Wikipedia writer asking questions to a topic expert, and the expert providing answers grounded on trusted Internet sources. This process is repeated multiple times to gather information and create a comprehensive outline.\n",
      "\n",
      "STORM's role as a writing system is to assist in the creation of grounded, long-form articles by automating the pre-writing stage. It uses a novel multi-stage approach to conduct research, create an outline, and generate a full-length article.\n",
      "\n",
      "STORM's effectiveness is demonstrated through its ability to outperform baseline approaches in terms of outline and article quality. The system's ability to ask effective questions and simulate conversations with experts enables it to gather more comprehensive and accurate information, leading to better article quality.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. The STORM paradigm is designed for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. \n",
      "\n",
      "In the study, STORM works by first discovering different perspectives on a topic by surveying existing articles and using these perspectives to guide question asking. It then simulates conversations between a Wikipedia writer and a topic expert, where the writer asks questions and the expert provides answers grounded on trustworthy online sources. \n",
      "\n",
      "The conversation simulation involves multiple turns, with the writer asking follow-up questions based on the expert's answers. The collected information from these conversations is then used to create an outline, which can be expanded into a full-length article. \n",
      "\n",
      "STORM's role as a writing system is to facilitate the generation of grounded, long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. It aims to assist individuals in initiating in-depth learning about a topic and reduce the time and effort required for expository writing. \n",
      "\n",
      "The system's effectiveness is evaluated using a dataset of recent, high-quality Wikipedia articles, and the results show that STORM outperforms other approaches in terms of outline quality and article breadth. Expert feedback also highlights the potential of STORM in generating well-organized and informative articles. \n",
      "\n",
      "Overall, STORM is a novel system that leverages large language models to automate the pre-writing stage and generate high-quality articles, making it a valuable tool for writers and researchers.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study to me in detail.\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20611cd4-e220-4111-9d65-a3c0fb168322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai_ra",
   "language": "python",
   "name": "broai_ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
