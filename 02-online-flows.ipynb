{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d61a8a4-6a32-4254-80c5-de4388f227a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cd0c3-0ab1-42c1-9644-1f670184673e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04460b5-436c-42de-947f-6479c9200be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"./memories.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7bfb94-451a-4ef8-a6e9-ca69453770c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from broai.prompt_management.core import PromptGenerator\n",
    "from broai.prompt_management.interface import Persona, Instructions, Examples, Example\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from broai.experiments.bro_agent import BroAgent\n",
    "import json\n",
    "from broai.interface import Context, Contexts\n",
    "from broai.experiments.vector_store import DuckVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90163eda-1ed4-4265-9183-c93a957ba808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.jargon_store import JargonStore, JargonRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea043c71-d617-4ab9-a351-69d1043f2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7751/2186289470.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: ReRanker\n",
      "  rr = ReRanker()\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.cross_encoder import ReRanker\n",
    "rr = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb15de4-b7c4-4a32-8c33-57d4a5588d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7751/2466456318.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  baai_em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 172132.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from broai.experiments.huggingface_embedding import BAAIEmbedding, EmbeddingDimension\n",
    "baai_em = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ee5a28-a41d-4cb6-86db-f3c979438387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7751/1440006260.py:1: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_7751/1440006260.py:2: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
      "/tmp/ipykernel_7751/1440006260.py:3: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: DuckVectorStore\n",
      "  longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n"
     ]
    }
   ],
   "source": [
    "raw_memory = DuckVectorStore(db_name=DB_NAME, table=\"raw_memory\", embedding=baai_em)\n",
    "enrich_memory = DuckVectorStore(db_name=DB_NAME, table=\"enrich_memory\", embedding=baai_em)\n",
    "longterm_memory = DuckVectorStore(db_name=DB_NAME, table=\"longterm_memory\", embedding=baai_em)\n",
    "jargon_memory = JargonStore(db_name=DB_NAME, table=\"jargon_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce04192-42f7-44c5-a0f7-b09280c5f262",
   "metadata": {},
   "source": [
    "# Agent Flows: \n",
    "- JargonDetector\n",
    "- JargonEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a806cf1-49d3-4483-a24e-515d0d28da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.jargon_detector import JargonDetector, InputMessage\n",
    "from agents.jargon_editor import JargonEditor, InputEditMessage\n",
    "from agents.query_decomposer import QueryDecomposer, InputMessage\n",
    "from agents.oracle import Oracle, InputOracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d545c43f-7d0c-47da-be81-c817a45ad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_conversation(original_message, model_name=\"us.meta.llama3-2-11b-instruct-v1:0\"):\n",
    "    potential_jargons = JargonDetector.run(request=InputMessage(message=original_message))\n",
    "    detected_jargons = [j for j in potential_jargons.jargons if j.confidence>.5]\n",
    "    jargon_knowledges = []\n",
    "    for j in detected_jargons:\n",
    "        jk = jargon_memory.fulltext_search(search_query=\"STORM\")\n",
    "        jargon_knowledges.extend(jk)\n",
    "    jargon_knowledges_str = \"\\n\\n\".join([f\"{enum+1}: {j.jargon}\\nEvidence: {j.evidence}\\nExplanation: {j.explanation}\" for enum, j in enumerate(jargon_knowledges)])\n",
    "    edited_message = JargonEditor.run(InputEditMessage(knowledge=jargon_knowledges_str, message=original_message))\n",
    "    sub_queries = QueryDecomposer.run(InputMessage(message=edited_message.edited_message))\n",
    "    retreived_contexts = []\n",
    "    for sq in sub_queries.sub_queries:\n",
    "        rc = longterm_memory.vector_search(search_query=sq, limit=10)\n",
    "        retreived_contexts.extend(rc)\n",
    "    id_list = []\n",
    "    deduplicated_contexts = []\n",
    "    for c in retreived_contexts:\n",
    "        if c.id not in id_list:\n",
    "            id_list.append(c.id)\n",
    "            deduplicated_contexts.append(c)\n",
    "    reranked_contexts, scores = rr.run(search_query=edited_message.edited_message, contexts=deduplicated_contexts, top_n=10)\n",
    "    prior_knowledge = \"\\n\\n\".join([f\"{c.context}\" for c in reranked_contexts])\n",
    "    Oracle.model.model_name = model_name\n",
    "    # answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "    answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=\", \".join(sub_queries.sub_queries)))\n",
    "    return answer.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495c4c54-1818-40ac-aee0-baa17b68f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PotentialJargon(jargon='STORM', confidence=0.8)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_message = \"What does STORM do in the research study?\"\n",
    "\n",
    "potential_jargons = JargonDetector.run(request=InputMessage(message=original_message))\n",
    "detected_jargons = [j for j in potential_jargons.jargons if j.confidence>.5]\n",
    "detected_jargons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8f498c-6d85-45d5-b523-9dd4cc76599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargon_knowledges = []\n",
    "for j in detected_jargons:\n",
    "    jk = jargon_memory.fulltext_search(search_query=\"STORM\")\n",
    "    jargon_knowledges.extend(jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd089142-3db7-48e4-ba38-55bbba916b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: STORM\n",
      "Evidence: We present STORM to automate the pre-writing stage\n",
      "Explanation: STORM is a system that automates the pre-writing stage\n",
      "\n",
      "2: STORM\n",
      "Evidence: we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "Explanation: STORM is a paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "\n",
      "3: STORM\n",
      "Evidence: STORM simulates a conversation between a Wikipedia writer and a topic expert\n",
      "Explanation: STORM is a system that simulates conversations\n",
      "\n",
      "4: STORM\n",
      "Evidence: STORM discovers different perspectives by surveying existing articles from similar topics\n",
      "Explanation: STORM is a system or method for discovering perspectives and controlling question asking process\n",
      "\n",
      "5: STORM\n",
      "Evidence: We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n",
      "Explanation: STORM is a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking\n"
     ]
    }
   ],
   "source": [
    "jargon_knowledges_str = \"\\n\\n\".join([f\"{enum+1}: {j.jargon}\\nEvidence: {j.evidence}\\nExplanation: {j.explanation}\" for enum, j in enumerate(jargon_knowledges)])\n",
    "print(jargon_knowledges_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2cfa7d2-d33c-41be-a1fd-a4d442a27ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does STORM, a system that automates the pre-writing stage, simulates conversations, discovers different perspectives, and is a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, do in the research study?\n"
     ]
    }
   ],
   "source": [
    "edited_message = JargonEditor.run(InputEditMessage(knowledge=jargon_knowledges_str, message=original_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2c70d0c-249b-4272-8196-6a17b914f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does STORM do in the research study?\n",
      "==========\n",
      "What does STORM, a system that automates the pre-writing stage, simulates conversations, discovers different perspectives, and is a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, do in the research study?\n"
     ]
    }
   ],
   "source": [
    "print(original_message)\n",
    "print(\"=\"*10)\n",
    "print(edited_message.edited_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2362d58-ee28-4668-b300-751c79df3d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecomposedQueries(sub_queries=['What does STORM do in the research study', 'STORM in the research study'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryDecomposer.run(InputMessage(message=original_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56228a3-7501-4a41-9f57-5ef82d274d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What does STORM do',\n",
       " 'STORM system',\n",
       " 'Automated pre-writing stage',\n",
       " 'Simulates conversations',\n",
       " 'Discovers different perspectives',\n",
       " 'Writing system for Synthesis of Topic Outlines',\n",
       " 'Retrieval and Multi-perspective Question Asking']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_queries = QueryDecomposer.run(InputMessage(message=edited_message.edited_message))\n",
    "sub_queries.sub_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e92889-96d6-4595-8a33-ef981676141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_contexts = []\n",
    "for sq in sub_queries.sub_queries:\n",
    "    rc = longterm_memory.vector_search(search_query=sq, limit=10)\n",
    "    retreived_contexts.extend(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d766e7b-fe0c-444e-b1c1-126e2907ff9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retreived_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0caf2fc6-3959-4381-ae9e-dedbebd26927",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "deduplicated_contexts = []\n",
    "for c in retreived_contexts:\n",
    "    if c.id not in id_list:\n",
    "        id_list.append(c.id)\n",
    "        deduplicated_contexts.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13a80cf0-2343-405b-bf09-c0d0d08c48dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retreived_contexts), len(deduplicated_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9bef55f-1257-4d02-9788-ec58fdc93463",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_knowledge = \"\\n\\n\".join([f\"{c.context}\" for c in deduplicated_contexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2acd700-513e-4a5e-bc20-88e3b0522f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM is a system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. It discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. STORM prompts an LLM to generate a list of related topics and subsequently extracts the tables of contents from their corresponding Wikipedia articles, if such articles can be obtained through Wikipedia API. These tables of contents are concatenated to create a context to prompt the LLM to identify N perspectives P = {p1, ..., p<sup>N</sup> } that can collectively contribute to a comprehensive article on t. STORM creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-2-3b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b8cfe34-51d0-493c-9b8a-9ccb1551f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM simulates conversations between a Wikipedia writer and a topic expert, discovers different perspectives by surveying articles, and uses these perspectives to control question asking, prompting an LLM to generate a list of related topics and extract tables of contents to identify N perspectives that contribute to a comprehensive article.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-2-11b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad216660-2900-4b08-b747-075520367fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM simulates conversations between a Wikipedia writer and a topic expert, discovers different perspectives by surveying articles, and creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge.\n"
     ]
    }
   ],
   "source": [
    "Oracle.model.model_name = \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "answer = Oracle.run(InputOracle(prior_knowledge=prior_knowledge, message=edited_message.edited_message))\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2a47af6-b519-4f3e-9e22-aaaf4b766d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    \"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "    \"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "    \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1000e7fc-4266-423d-afe7-04b62e4cdd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. It discovers different perspectives by surveying articles, uses them to control question asking, and prompts an LLM to generate a list of related topics and extract tables of contents to identify N perspectives that contribute to a comprehensive article. STORM simulates a conversation between a Wikipedia writer and a topic expert to generate questions and answers, using LLM and trusted sources to ensure factual information. It creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage by discovering different perspectives, simulating conversations, and creating an outline for long-form articles. It uses LLMs to ask incisive questions, retrieve trusted information from the Internet, and synthesize the collected information to create a comprehensive outline.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a writing system that automates the pre-writing stage by discovering different perspectives, simulating conversations between a writer and an expert, and creating an outline for long-form articles. It researches a topic, generates questions, and retrieves trusted information from the Internet to create a comprehensive outline that can be expanded into a full-length article.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"What does STORM do in the research study?\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45fc33ba-b09e-4000-805d-e14cf12f69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "STORM uses a dataset of recent Wikipedia articles, specifically the FreshWiki dataset, to discover different perspectives on a given topic. It then simulates conversations between a writer and an expert grounded on trustworthy online sources to generate a comprehensive outline. The outline is refined using the LLM's intrinsic knowledge and the gathered conversations from different perspectives. STORM is a framework that automates the pre-writing stage by discovering perspectives, simulating information-seeking conversations, and creating a comprehensive outline for long-form articles.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "The dataset used in the STORM study is called FreshWiki, which is a dataset of recent high-quality Wikipedia articles.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "FreshWiki, a dataset of recent high-quality Wikipedia articles.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"What does the dataset used in the study?\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "359be844-d9ed-471c-bd22-b73da45edf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "STORM is a framework that automates the pre-writing stage by discovering different perspectives, simulating information-seeking conversations, and creating a comprehensive outline. It uses a pseudo code that includes steps such as researching a topic, creating an outline, and simulating conversations between a writer and an expert to generate a full-length article. STORM uses a large language model to generate questions and answers, and it also uses trusted sources from the internet to ground the answer to each query. The framework is designed to assist the creation of grounded, long-form articles, and it has been evaluated in a study that shows it outperforms other approaches in terms of heading soft recall, entity recall, and full-length article quality.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a framework that automates the pre-writing stage by discovering perspectives, simulating conversations, and creating outlines for long-form articles. It discovers different perspectives by surveying articles and uses them to control question asking, prompting an LLM to generate a list of related topics and extract tables of contents to identify N perspectives that contribute to a comprehensive article. STORM simulates conversations between a Wikipedia writer and a topic expert to generate questions and answers, using LLM and trusted sources to ensure factual information. It creates an outline for an article by generating a draft outline from a topic and refining it with simulated conversations and LLM knowledge. STORM's role in Synthesis of Topic Outlines is to synthesize the collected information to create an outline, and its function in Retrieval and Multi-perspective Question Asking is to discover different perspectives by surveying articles and use them to control question asking.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM works by discovering different perspectives on a topic, simulating conversations between a writer and an expert, and creating a comprehensive outline. Its role is in the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking, where it automates the pre-writing stage by researching a topic, creating an outline, and simulating conversations to generate a full-length article.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"How does STORM work in the study?\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d6deaa0-3903-454c-8500-a1241f68ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "STORM is a novel system that automates the pre-writing stage by researching the topic and creating an outline using LLMs to ask incisive questions and retrieving trusted information from the Internet. It discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. STORM simulates a conversation between a Wikipedia writer and a topic expert to generate questions and answers, using LLM and trusted sources to ensure factual information. The system generates an outline and references for a given topic, considering multiple perspectives and simulated conversations.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM works by discovering different perspectives through article surveys, simulating conversations between Wikipedia writers and topic experts, and automating the pre-writing stage. It uses LLMs to ask incisive questions, retrieve trusted information from the Internet, and create an outline for an article. STORM's pre-writing stage involves researching the topic, creating an outline, and simulating conversations to generate a comprehensive article.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM works by automating the pre-writing stage for creating Wikipedia-like articles from scratch. It discovers different perspectives by surveying existing articles, simulates conversations between Wikipedia writers and topic experts, and creates an outline by using LLMs to ask incisive questions and retrieving trusted information from the Internet. The process involves researching the topic, identifying perspectives, simulating multi-turn conversations, and refining the outline to produce a full-length article.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study.\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8cd3fff9-fd86-4644-a894-2e92d9c57bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "STORM is a framework that automates the pre-writing stage by discovering different perspectives, simulating information-seeking conversations, and creating comprehensive outlines for long-form articles. It uses large language models to ask incisive questions and retrieve trusted information from the Internet, and then creates an outline that can be expanded into a full-length article. STORM has been evaluated and found to outperform other approaches in terms of outline and article quality, and has been shown to be effective in generating grounded and organized long-form articles.\n",
      "====================\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "STORM is a framework that automates the pre-writing stage by discovering perspectives, simulating conversations, and creating outlines for long-form articles. It uses large language models to ask incisive questions, retrieve trusted information from the Internet, and generate a comprehensive outline. STORM's pre-writing stage involves discovering diverse perspectives, simulating conversations, and creating an outline. The framework uses a novel multi-stage approach to generate a list of related topics, extract tables of contents, and identify N perspectives that contribute to a comprehensive article. STORM also simulates conversations between a Wikipedia writer and a topic expert to generate questions and answers, using LLM and trusted sources to ensure factual information. The framework has been evaluated using the FreshWiki dataset and has shown improved performance in generating outlines and full-length articles compared to RAG and oRAG.\n",
      "====================\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "STORM is a framework that automates the pre-writing stage by discovering perspectives, simulating conversations, and creating outlines for long-form articles. It researches a topic, creates an outline, and simulates conversations between a writer and an expert to generate a full-length article. STORM uses large language models to ask incisive questions and retrieve trusted information from the Internet, and it has been shown to outperform other methods in terms of outline quality and full-length article generation.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study in plain English.\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3eb8154a-591a-4401-8559-ef4ea05a3c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/broai/experiments/bro_agent.py:57: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: content_extractor\n",
      "  return self.content_extractor(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for PotentialJargons\n",
      "  Invalid JSON: trailing characters at line 41 column 1 [type=json_invalid, input_value='{\\n    \"$defs\": {\\n     ....8\\n        }\\n    ]\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n",
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for DecomposedQueries\n",
      "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value=\"**Simple Explanation of ...ike cells or molecules.\", input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n",
      "model: us.meta.llama3-2-3b-instruct-v1:0\n",
      "The STORM implementation uses zero-shot prompting with the DSPy framework, achieving better results in automatic article quality evaluation and heading recall compared to other models. However, the generated articles contain emotional language, lack neutrality, and have issues with red herring fallacy and citation quality due to biased source material and special words. Improper inferential linking, inaccurate paraphrasing, and citing irrelevant sources are the most common errors in the generated articles. The study suggests that future research in grounded text generation should focus on preventing LLMs from making overly inferential leaps based on the provided information.\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/broai/experiments/bro_agent.py:57: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: content_extractor\n",
      "  return self.content_extractor(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for PotentialJargons\n",
      "  Invalid JSON: trailing characters at line 41 column 1 [type=json_invalid, input_value='{\\n    \"$defs\": {\\n     ....8\\n        }\\n    ]\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n",
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for DecomposedQueries\n",
      "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value=\"**Simple Explanation of ...ike cells or molecules.\", input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n",
      "model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "The STORM system evaluates outline quality using GPT-3.5 and GPT-4 models, achieving significant improvements in outline quality and article generation, with GPT-4 outperforming GPT-3.5 in most tasks. However, the generated articles by STORM contain emotional language, lack neutrality, and have issues with red herring fallacy and citation quality due to biased source material and special words. The primary issue raised is that the generated articles often contain emotional language and lack neutrality, primarily due to the source material. STORM currently retrieves grounding sources from the Internet which is not neutral and contains considerable promotional content on its own. Addressing this bias in the pre-writing stage represents a valuable direction for future research. Additionally, the study examines error types in generated text, including improper inferential linking, inaccurate paraphrasing, and citing irrelevant sources, with a focus on preventing LLMs from making overly inferential leaps.\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/broai-researcher-assistant/.venv/lib/python3.11/site-packages/broai/experiments/bro_agent.py:57: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: content_extractor\n",
      "  return self.content_extractor(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for PotentialJargons\n",
      "  Invalid JSON: trailing characters at line 41 column 1 [type=json_invalid, input_value='{\\n    \"$defs\": {\\n     ....8\\n        }\\n    ]\\n}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n",
      "\u001b[91mBoth parse_structured_output and content_extractor failed:\n",
      "1 validation error for DecomposedQueries\n",
      "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value=\"**Simple Explanation of ...ike cells or molecules.\", input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "BroAgent.parse_structured_output() got multiple values for argument 'text'\u001b[0m\n",
      "model: us.meta.llama3-3-70b-instruct-v1:0\n",
      "The study examines error types in generated text, including improper inferential linking, inaccurate paraphrasing, and citing irrelevant sources, with a focus on preventing LLMs from making overly inferential leaps.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "original_message = \"Explain how STORM works in the study to me like I'm a five years old.\"\n",
    "for m in model_list:\n",
    "    answer = batch_conversation(original_message=original_message, model_name=m)\n",
    "    print(\"model:\", m)\n",
    "    print(answer)\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9af7c-d5a5-4454-ab6e-c9e16584766f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai-ra",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
