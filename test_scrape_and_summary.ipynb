{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe07506-095a-4315-a7ec-42a26563b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e491c7-53dd-4549-b2e9-9e22921fe47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = [\n",
    "#     \"https://medium.com/data-science/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883\",\n",
    "#     \"https://medium.com/@lmpo/a-brief-history-of-ai-with-deep-learning-26f7948bc87b\" # membership\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "804d60f6-d06a-4cef-9a3c-a66be3ea8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://medium.com/data-science/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a975df-03e6-4dda-a4de-663ee7a4d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def scrape_jina_ai(url:str)->str:\n",
    "    response = requests.get(\"https://r.jina.ai/\"+url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f330f824-b023-4ca4-afe9-169761897f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.79 ms, sys: 1 μs, total: 9.79 ms\n",
      "Wall time: 721 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = scrape_jina_ai(url=urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fe1ccc-5ea9-4146-988c-b62721cdf3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Introducing Deep Learning and Neural Networks — Deep Learning for Rookies (1)\n",
      "\n",
      "URL Source: https://medium.com/data-science/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883\n",
      "\n",
      "Published Time: 2017-06-18T15:21:33.893Z\n",
      "\n",
      "Markdown Content:\n",
      "![Image 1](https://miro.medium.com/v2/resize:fit:700/1*R85xoznnAH7kF1YIL3ME5w.jpeg)\n",
      "\n",
      "Source: Allison Linn, Microsoft\n",
      "\n",
      "[![Image 2: Nahua Kang](https://miro.medium.com/v2/resize:fill:32:32/1*ZFG3jWAGZKKGtBJohEuF-A.jpeg)](https://medium.com/@nahua?source=post_page---byline--bd68f9cf5883---------------------------------------)\n",
      "\n",
      "21 min read\n",
      "\n",
      "Jun 18, 2017\n",
      "\n",
      "_Follow me on_ [_Twitter_](https://twitter.com/nahuakang) _to learn more about life in a Deep Learning Startup._\n",
      "\n",
      "Welcome to the first post of my series **Deep Learning for Rookies** by me, a rookie. I’m writing as a _reinforcement learning_ strategy to process and digest the knowledge better. But if you are a deep learning rookie, then this is for you as well because **we can learn together as rookies**!\n",
      "\n",
      "(_You can also read_ [_this post on my website_](http://nahuakang.com/2017/06/21/deep-learning-for-rookies-1/)_, which supports LaTeX with MathJax_)\n",
      "\n",
      "![Image 3](https://miro.medium.com/v2/resize:fit:600/1*GyVlHXgV8q8-bL48djJZPQ.jpeg)\n",
      "\n",
      "Source: deepinstinct.com\n",
      "\n",
      "Deep learning is probably one of the hottest tech topics right now. Large corporations and young startups alike are all gold-rushing this fancy field. If you think big data is important, then you should care about deep learning. [The Economist](https://medium.com/u/bea61c20259e?source=post_page---user_mention--bd68f9cf5883---------------------------------------) says that [data is the new oil](http://www.economist.com/news/briefing/21721634-how-it-shaping-up-data-giving-rise-new-economy) in the 21st Century. If data is the crude oil, databases and data warehouses are the drilling rigs that digs and pumps the data on the internet, then think of deep learning as the oil refinery that finally turns crude oil into all the useful and insightful final products. There could be a lot of “fossil fuels” hidden underground, and there are a lot of drills and pumps in the market, but without the right refinery tools, you ain’t gonna get anything valuable. That’s why deep learning is important. It’s part of the data-driven big picture.\n",
      "\n",
      "The good news is, we are not going to run out of data and our “refinery machine” is getting better and better. Today, just about doing anything online will generate data. So data, unlike oil, is “sustainable” and growing “explosively”. In the meantime, as long as the data isn’t garbage-in, then there’s no garbage-out from deep learning. Hence the more data, the merrier. (Check out [Trent McConaghy](https://medium.com/u/f1cb98e196bc?source=post_page---user_mention--bd68f9cf5883---------------------------------------)’s post on [blockchains for AI](https://blog.bigchaindb.com/blockchains-for-artificial-intelligence-ec63b0284984) as a solution for data to have reputation!).\n",
      "\n",
      "Also, this “oil refinery” is improving on both software and hardware. Deep learning algorithms have improved over the past few decades and developers around the world have contributed to open source frameworks like TensorFlow, Theano, Keras, and Torch, all of which make it easy for people to build deep learning algorithms as if playing with LEGO pieces. And thanks to the demand from gamers around the world, GPUs (graphics processing units) make it possible for us to leverage deep learning algorithms to build and train models with impressive results in a time-efficient manner! So to all the parents who don’t like your kids playing games: Gaming has its silver lining…\n",
      "\n",
      "Deep Learning: The Secret Recipe\n",
      "--------------------------------\n",
      "\n",
      "You’ve probably read on the news and know that deep learning is the secret recipe behind many exciting developments and has made many of our wildest dreams and perhaps also nightmares come true. Who would have thought that DeepMind’s AlphaGo could defeat Lee Sedol, one of the best Go players, in the deepest board game which boasts more possible moves than there are atoms in the entire universe? A lot of people, including me, never saw it coming. It seemed impossible. But it’s here now. Deep learning is beating us in the most challenging board game. When is the AI awakening? Some think it will be soon.\n",
      "\n",
      "![Image 4](https://miro.medium.com/v2/resize:fit:700/1*7UPoSbQ_uNq-AiZ-TR5_dg.jpeg)\n",
      "\n",
      "Lee Sedol vs. AlphaGo in 2016, Source: The New Yorker\n",
      "\n",
      "And we haven’t talked about the other impressive applications powered by deep learning, such as Google Translate and Mobileye’s autonomous driving. You name it. Did I forget to mention that [deep learning is also beating physicians at diagnosing cancer](http://www.newyorker.com/magazine/2017/04/03/ai-versus-md)? Deep learning excels at many tasks with lower error rates than humans! It’s not just automating the boring stuff, but also the fun stuff. Sigh, we mundane humans…\n",
      "\n",
      "> Dear Mr. President, it’s not foreigners. It’s automation.\n",
      "\n",
      "Here’s a short list of general tasks that deep learning can perform in real situations:\n",
      "\n",
      "1.  Identify faces (or more generally image categorization)\n",
      "2.  Read handwritten digits and texts\n",
      "3.  Recognize speech (no more transcribing interviews yourself)\n",
      "4.  Translate languages\n",
      "5.  Play computer games\n",
      "6.  Control self-driving cars (and other types of robots)\n",
      "\n",
      "And there’s more. Just pause for a second and imagine all the things that deep learning could achieve. It’s amazing and perhaps a bit scary!\n",
      "\n",
      "Distinctions: AI, Machine Learning, and Deep Learning\n",
      "-----------------------------------------------------\n",
      "\n",
      "Okay, wait a second. You’ve probably seen terms like Artificial Intelligence (**AI**), Machine Learning (**ML**), and Deep Learning (**DL**) flying all over the place in your social media newsfeed. What’s the difference between all of them? Do they mean the same thing or what? Great question. Before we go deeper into deep learning, it is important to gradually build a conceptual framework of these jargons.\n",
      "\n",
      "Roughly speaking, the graph below demonstrates the relationship for these 3 concepts. Deep learning is a subfield of Machine Learning, and Machine Learning is a subfield of Artificial Intelligence. Both [Ophir Samson](https://medium.com/u/eab3d47a7df7?source=post_page---user_mention--bd68f9cf5883---------------------------------------) and [Carlos E. Perez](https://medium.com/u/1928cbd0e69c?source=post_page---user_mention--bd68f9cf5883---------------------------------------) have written good stories about them. Check [here](https://medium.com/towards-data-science/deep-learning-weekly-piece-the-differences-between-ai-ml-and-dl-b6a203b70698) and [here](https://medium.com/intuitionmachine/why-deep-learning-is-radically-different-from-machine-learning-945a4a65da4d).\n",
      "\n",
      "![Image 5](https://miro.medium.com/v2/resize:fit:700/1*vdHyRQfjt9__LKGgJsC2Kg.png)\n",
      "\n",
      "Source: Nvidia\n",
      "\n",
      "Let’s discuss the all encompassing AI first. You probably know the **Turing Test** already. A computer passes the Turing Test if a human, after posing some written questions to the computer, cannot tell whether the written responses come from another human or the computer. According to _Artificial Intelligence: A Modern Approach_, Peter Norvig and Stuart Russell define the 4 capabilities a computer must command in order to pass the Turing Test:\n",
      "\n",
      "*   **Natural Language Processing:** to communicate successfully in English\n",
      "*   **Knowledge Representation:** to store what the computer reads\n",
      "*   **Automated Reasoning:** to use the stored knowledge to answer questions and draw new conclusions\n",
      "*   **Machine Learning:** to adapt to new circumstances and to identify new patterns\n",
      "\n",
      "Ah, there’s the term “machine learning”! ML is about training the learning algorithms like Linear Regression, KNN, K-Means, Decision Trees, Random Forest, and SVM with datasets, so that the algorithms could learn to adapt to a new situation and find patterns that might be interesting and important. Again, ML is data-driven. A lot of strange terms for the learning algorithms? No worries, I don’t know all of them either. So we’ll learn together in the future.\n",
      "\n",
      "For training ML, the dataset can be labeled, e.g. it comes with an “answer sheet”, telling the computer what the right answer is, like which emails are spams and which are not. This is called a **supervised learning** and algorithms like Linear Regression and KNN are used for such supervised **regression** or **classification**. Other datasets might not be labeled, and you are literally telling the algorithm such as K-Means to **associate** or **cluster** patterns that it finds without any answer sheet. This is called **unsupervised learning**. Here’s a [great answer to supervised vs. unsupervised learning](https://stackoverflow.com/a/1854449/6297414) on Stack Overflow and here’s also a [post on supervised vs unsupervised](http://oliviaklose.com/machine-learning-2-supervised-versus-unsupervised-learning/) from [Olivia Klose](https://medium.com/u/2b6a75cd1519?source=post_page---user_mention--bd68f9cf5883---------------------------------------)’s blog.\n",
      "\n",
      "![Image 6](https://miro.medium.com/v2/resize:fit:700/1*3nfPT9oOadXZGpPSy6h7vQ.png)\n",
      "\n",
      "Source: [http://oliviaklose.com/](http://oliviaklose.com/)\n",
      "\n",
      "Norvig and Russell also mention another test called the **Total Turing Test**, which further examines the computer’s perceptual abilities with physical simulation. To pass this one, the computer needs:\n",
      "\n",
      "*   **Computer Vision:** to perceive objects in the surroundings\n",
      "*   **Robotics:** to manipulate objects and move around\n",
      "\n",
      "So what about DL now? Remember the general tasks that deep learning is good at from the section above? Things like face or handwritten text recognition have to do with computer vision because you are feeding in graphics to the computer for analysis. Other tasks like language translation or speech recognition have to do with natural language processing (NLP). So DL is a sub-branch of ML in that it also has a set of learning algorithms that can train on and learn from data, and more specifically **DL is powered by neural networks**. Moreover, DL can perform outside the machine learning area and comes to assist other areas like computer vision and NLP so that hopefully AI could pass the Turing Test and Total Turing Test one day!\n",
      "\n",
      "But what the heck is a neural network? Is it imitating the behaviors of actual neuron cells? Or is it some sort of a magical black box? For some of you, the information provided so far might feel a bit too much, so let’s take a break and check out some free online resources to see which suits you :) Afterwards, we will go straight into neural networks.\n",
      "\n",
      "Half-way Bonus: Valuable Resources\n",
      "----------------------------------\n",
      "\n",
      "Neural networks and DL are often hidden behind a mysterious veil. All the technical jargons related to the topic might make beginners deeply confused. Since DL will automate many tasks and replace many workers in the future, I personally believe it is important that we all keep an open mind and curiosity to learn new technology. DL can replace a worker who works on manual, repetitive tasks. But DL cannot replace the scientist or the engineer building and maintaining a DL application.\n",
      "\n",
      "Currently there are already many great courses, tutorials, and books on the internet covering this topic, such as (not exhaustive or in specific order):\n",
      "\n",
      "1.  Michael Nielsen’s [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
      "2.  Geoffrey Hinton’s [Neural Networks for Machine Learning](https://www.coursera.org/learn/neural-networks)\n",
      "3.  Goodfellow, Bengio, & Courville’s [Deep Learning](http://www.deeplearningbook.org/)\n",
      "4.  Ian Trask’s [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning),\n",
      "5.  Francois Chollet’s [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n",
      "6.  [Udacity](https://medium.com/u/2929690a28fb?source=post_page---user_mention--bd68f9cf5883---------------------------------------)’s [Deep Learning Nanodegree](https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101) (not free but high quality)\n",
      "7.  [Udemy’s Deep Learning A-Z](https://www.udemy.com/deeplearning/learn/v4/overview) ($10–$15)\n",
      "8.  Stanford’s [CS231n](http://cs231n.stanford.edu/index.html) and [CS224n](http://web.stanford.edu/class/cs224n/)\n",
      "9.  Siraj Raval’s [YouTube channel](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)\n",
      "\n",
      "The list goes on and on. [David Venturi](https://medium.com/u/b3eb78490b02?source=post_page---user_mention--bd68f9cf5883---------------------------------------) has a post for [freeCodeCamp](https://medium.com/u/8b318225c16a?source=post_page---user_mention--bd68f9cf5883---------------------------------------) that lists many more resources. Check it out [here](https://medium.freecodecamp.com/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0).\n",
      "\n",
      "We’re truly blessed in the age of self-education. By the way, have you heard about the high school student, Abu Qader, from Chicago? This kid taught himself machine learning and Tensorflow, a deep learning framework, and helped improve [the diagnosis of breast cancer to 93%-99% real-time accuracy](http://chicagoinno.streetwise.co/2016/07/13/this-chicago-high-school-student-uses-artificial-intelligence-to-make-smarter-breast-cancer-diagnoses/)! He was featured on Google I/O 2017. Below is an inspiring video of his story from Google.\n",
      "\n",
      "Source: Google\n",
      "\n",
      "Perceptron: The Prelude of DL\n",
      "-----------------------------\n",
      "\n",
      "Alright, I hope Abu Qader’s story makes you excited about learning! Let’s get to the second main topic of this post: an introduction to neural networks. The ancient Chinese philosopher Lao Tzu once said:\n",
      "\n",
      "> “The journey of a thousand miles begins with one step.”\n",
      "\n",
      "So we will begin and end this post with a very simple neural network. Sounds cool? Wunderbar.\n",
      "\n",
      "Despite its new-found fame, the field of neural networks isn’t new at all. In 1958, [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt#Perceptron), an American psychologist, attempted to build “[a machine which senses, recognizes, remembers, and responds like the human mind](http://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence)” and called the machine a **Perceptron**. But Rosenblatt didn’t invent perceptrons out of thin air. Actually, he stood on the shoulders of giants and was inspired by previous works from Warren McCulloch and Walter Pitts in the 1940s. Gosh, that makes neural networks or, more specifically perceptron, a dinosaur in this fast-changing world of technology.\n",
      "\n",
      "![Image 7](https://miro.medium.com/v2/resize:fit:233/1*g_WT9YsMyb9ivNALW50xQw.jpeg)\n",
      "\n",
      "Rosenblatt and Perceptron, Source: The New Yorker\n",
      "\n",
      "So let’s see what a perceptron is. First, have a look at the neuron cell below: the **dendrites** are extensions of the nerve cell (in the left lower corner of the graph). They receive signals and then transmit the signals to the cell body, which processes the stimulus and decide whether to trigger signals to other neuron cells. In case this cell decides to trigger signals, the extension on the cell body called **axon** will triggers chemical transmission at the end of the axon to other cells. You don’t have to memorize anything here. We are not studying neuroscience, so a vague impression of how it works will be enough.\n",
      "\n",
      "![Image 8](https://miro.medium.com/v2/resize:fit:700/1*z0sLgHIGhjFG1ugErib30Q.jpeg)\n",
      "\n",
      "Original Source: thinglink.com\n",
      "\n",
      "![Image 9](https://miro.medium.com/v2/resize:fit:700/1*ZoT34FHnrrPxWHsXv6mK8Q.jpeg)\n",
      "\n",
      "A Single Perceptron\n",
      "\n",
      "Now, above is a graph of how a perceptron looks like. Pretty similar to the nerve cell graph above, right? Indeed. Perceptrons and other neural networks are inspired by real neurons in our brain. Note it’s only _inspired_ and does not work exactly like real neurons. The procedure of a perceptron processing data is as follows:\n",
      "\n",
      "1.  On the left side you have neurons (small circles) of _x_ with subscripts _1, 2, … , m_ carrying **data input.**\n",
      "2.  We multiply each of the input by a **weight** w, also labeled with subscripts _1, 2, …, m_, along the arrow (also called a **synapse**) to the big circle in the middle. So _w1 \\* x1_, _w2 \\* x2_, _w3 \\* x3_ and so on.\n",
      "3.  Once all the the inputs are multiplied by a weight, we sum all of them up and add another pre-determined number called **bias.**\n",
      "4.  Then, we push the result further to the right. Now, we have this **step function** in the rectangle. What it means is that if the result from step 3 is any number equal or larger than 0, then we get 1 as output, otherwise if the result is smaller than 0, we get0 as output.\n",
      "5.  The **output** is either 1 or 0.\n",
      "\n",
      "Note that alternatively, if you move bias to the right side of the equation in the activation function like _sum(wx) ≥ -b_ then this _\\-b_ is called a **threshold value**. So if the sum of the inputs and weights is greater than or equal to the threshold, then the activation triggers an 1. Otherwise, the activation outcome is 0. Choose whichever that helps you to understand better as these two ways of representation are interchangeable.\n",
      "\n",
      "![Image 10](https://miro.medium.com/v2/resize:fit:700/1*NZFlr7z4x5iO0OWGPElw-g.jpeg)\n",
      "\n",
      "Illustration of using Threshold Value instead of Bias\n",
      "\n",
      "I add another perceptron graph below, this time with each step colored. It’s very important that you fully understand it and remember what is happening at each step because we will ignore the middle steps in future graphs when we talk about more complicated neural network structures:\n",
      "\n",
      "![Image 11](https://miro.medium.com/v2/resize:fit:700/1*-oWHnqj0hjipXyeaUy8k8A.jpeg)\n",
      "\n",
      "Procedures of a Perceptron labeled in colors\n",
      "\n",
      "1.  **Inputs** are fed into the perceptron\n",
      "2.  **Weights** are multiplied to each input\n",
      "3.  **Summation** and then add **bias**\n",
      "4.  **Activation function** is applied. Note that here we use a step function, but there are other more sophisticated activation functions like **sigmoid, hyperbolic tangent (_tanh_), rectifier (_relu_) and more**. No worries, we will cover many of them in the future!\n",
      "5.  **Output** is either triggered as 1, or not, as 0. Note we use **_y hat_** to label output produced by our perceptron model\n",
      "\n",
      "In the future, we might sometimes simplify our perceptron as the following without mentioning steps 3 and 4. This type of perceptron we just talked about is also a **single-layer perceptron** since we process the inputs directly into an output without any more layers of neurons in the middle:\n",
      "\n",
      "![Image 12](https://miro.medium.com/v2/resize:fit:366/1*74YD-gADYB8xC7MQ36apFA.jpeg)\n",
      "\n",
      "Simplified Representation of Perceptron\n",
      "\n",
      "**Perceptron: Intuition**\n",
      "-------------------------\n",
      "\n",
      "Okay, you know how a perceptron works now. It’s just some mechanical multiplications followed by summation and then some activation…voilà you get an output. Yeah, how on earth is this anything close to neurons in human brain?\n",
      "\n",
      "For the sake of understanding perceptrons, let’s look at a very easy example that isn’t necessarily realistic. Suppose you are very motivated after reading my post and **you need to decide whether to study DL or not**. There are 3 factors that influence your decision:\n",
      "\n",
      "1.  If you will earn more money after mastering DL (Yes: 1, No: 0)\n",
      "2.  Is the relevant mathematics and programming easy (Yes: 1, No: 0)\n",
      "3.  You can work on DL immediately without the need for an expensive GPU (Yes: 1, No: 0)\n",
      "\n",
      "We use _x1, x2,_ and _x3_ as input variables for each of these factors and assign a binary value (1 or 0) to each of them since the answer is simply yes or no. Suppose you really like DL so far and you are willing to overcome your life-long fear of math and programming. And you also have some savings invest now in a pricey Nvidia GPU to train your DL models. Assume these two factors are equally less important as you can compromise on them. However, you really want to earn more money after spending so much time and energy in learning DL. So with a high expectation of return on investment, if you can’t earn more $$$ afterwards, you won’t waste your precious time on DL.\n",
      "\n",
      "With an understanding of your decision-making preference, let’s assume you have a 100% probability of earning more money after learning DL because there’s a lot of demand for very little supply in the market. So _x1 = 1_. Let’s say the math and programming is super hard. So _x2 = 0_. Finally, let’s say you must have a powerful GPU like Titan X. So _x3 = 0_. Ok, we have the inputs ready and can also initialize the weights. Let’s go for _w1 = 6, w2 = 2, w3 = 2_. **The larger the weight, the more influential the corresponding input is**. So since you value money the most for your decision to learn DL, _w1 \\> w2_ and _w1 \\> w3_.\n",
      "\n",
      "We’ll assume the threshold value _threshold = 5,_ which is equivalent to say that the bias term _bias = -5_. We add it all up and plus the bias term. Check the following for the process of determining whether you will learn DL or not by using a perceptron.\n",
      "\n",
      "![Image 13](https://miro.medium.com/v2/resize:fit:700/1*GQ5_FyA3jKqO248P9q3wIQ.jpeg)\n",
      "\n",
      "Note with a threshold value of 5, we will only learn deep learning if we earn more money. Even if both the math is easy (_x2 = 1_) and you don’t need to spend money buying a GPU (_x3 = 1_), you will still not study DL if you can’t earn more money later. See the illustration below:\n",
      "\n",
      "![Image 14](https://miro.medium.com/v2/resize:fit:700/1*YjH0VFjW_IYsK5-hh8UXAg.jpeg)\n",
      "\n",
      "Now you know the trick with bias/threshold. This high threshold of 5 means your dominating factor must be satisfied for the perceptron to trigger an output of 1. Otherwise, the output will be 0.\n",
      "\n",
      "Here’s the fun part: varying the weights and threshold/bias will result in different possible decision-making models. So if, for instance, we lower the threshold from _threshold = 5_ to _threshold = 3,_ then we have more possible scenarios for the output to be 1. Now, the _bare minimum requirement_ for _output = 1_ is:\n",
      "\n",
      "1.  You will earn more money afterwards, so _x1 = 1_ guarantees your decision to learn DL regardless of the values to _x2_ and _x3_\n",
      "2.  Or, the math is easy and you don’t need to buy GPU, so _x2 = x3 = 1_ also guarantees that you decide to learn DL regardless of the value to _x1_\n",
      "\n",
      "How so? You probably know already ;) Below is the explanation:\n",
      "\n",
      "![Image 15](https://miro.medium.com/v2/resize:fit:700/1*R-4inufD0ZfRKIB3KTIfdw.jpeg)\n",
      "\n",
      "Yup. The threshold is lower now so the other 2 factors could motivate you to learn DL even if your prospect of earning more money was gone. Now, I encourage you to play around with the weights _w1, w2, and w3_ and see how your decision on learning DL will change accordingly!\n",
      "\n",
      "Perceptron: Learning in Action\n",
      "------------------------------\n",
      "\n",
      "Why should you play around with the weights on this example? Because it helps you to understand how perceptron learns. Now, we’ll use this example together with the inputs and the weights to illustrate the single-layer perceptron and see what it can achieve despite its limitations.\n",
      "\n",
      "In a real DL model, we are given input data, which we can’t change. Meanwhile, the bias term is initialized before you train your neural networks model. So suppose we assume the bias is 7. Now, let’s assume the following input data so that (1). you will earn more money, (2). math and programming for DL will be hard, and (3). yes you have to spend $1400 for a GPU to work on DL, and _most importantly,_ **_we assume that you actually want to learn deep learning,_** _which we’ll name as the_ **_desired output_** _for how ideally the perceptron should correctly predict or determine_:\n",
      "\n",
      "![Image 16](https://miro.medium.com/v2/resize:fit:700/1*rFFTFkqbHau6PuOIWJDCoQ.jpeg)\n",
      "\n",
      "Let’s further assume that our weights are initialized at the following:\n",
      "\n",
      "![Image 17](https://miro.medium.com/v2/resize:fit:700/1*giZmWROYk520MJ4HtaH5Rw.jpeg)\n",
      "\n",
      "So with input data, bias, and the output label (desired output):\n",
      "\n",
      "![Image 18](https://miro.medium.com/v2/resize:fit:700/1*UZ_AOisLv3X7Fpo3V2ouEQ.jpeg)\n",
      "\n",
      "Okay, we know that the actual output from your neural network differs from your real decision of wanting to study DL. So what should the neural network do to help itself learn and improve, given this difference between actual and desired output? Yup, we can’t change input data, and we have initialized our bias now. So the only thing we can do is that we can tell the perceptron to adjust the weights! If we tell the perceptron to increase _w1_ to 7, without changing _w2_ and _w3_, then:\n",
      "\n",
      "![Image 19](https://miro.medium.com/v2/resize:fit:700/1*ZqSiECfrBqs4GQyX22Gpsg.jpeg)\n",
      "\n",
      "**Adjusting the weights is the key to the learning process of our perceptron**. And single-layer perceptron with step function can utilize the learning algorithm listed below to tune the weights after processing each set of input data. Try updating the weights with this algorithm yourself :) This is pretty much how a single-layer perceptron learns.\n",
      "\n",
      "![Image 20](https://miro.medium.com/v2/resize:fit:700/1*NEipz37gAVQN_6XqFjRLUg.jpeg)\n",
      "\n",
      "By the way, now as we finished our not-so-realistic example, I can tell you that you don’t need to buy a GPU. If you are training smaller datasets as a beginner, you most likely won’t need a GPU. However, you can use cloud services like [AWS](https://aws.amazon.com/), [Floyd](https://www.floydhub.com/), and possibly [Google TPU](https://cloud.google.com/tpu/)) instead of a real GPU when you start training larger datasets with a lot of image files.\n",
      "\n",
      "Also, the math for DL isn’t easy but also not insurmountable. Mostly, we will just encounter some matrix operations and basic calculus. But remember, nothing that makes you stand out is easy to learn. Here’s a quote from _Talent is Overated_:\n",
      "\n",
      "> “Doing things we know how to do well is enjoyable, and that’s exactly the opposite of what deliberate practice demands. Instead of doing what we’re good at, we should insistently seek out what we’re not good at. Then we identify the painful, difficult activities that will make us better and do those things over and over. If the activities that lead to greatness were easy and fun, then everyone would do them and they would not distinguish the best from the rest.”\n",
      "\n",
      "So to those of you who are scared of math and programming like I used to be, I hope this quote gives you some courage to keep learning and practicing :)\n",
      "\n",
      "Perceptron: Limitations\n",
      "-----------------------\n",
      "\n",
      "Despite some early sensations from the public, perceptron’s popularity faded away quietly because of its limitations. In 1969, Marvin Minsky and Seymour Papert discussed these limitations, including perceptron’s inability to learn an XOR (exclusive-or) gate (so basically a **_single-layer_ perceptron** with step function cannot understand the logic that the weather has to be either hot or cold, but not both). And these logic gates like AND, OR, NOT, XOR are very important concepts that are powering your computer ;) TutorialsPoint has a list of logic gates if you want to learn more. Check [here](https://www.tutorialspoint.com/computer_logical_organization/logic_gates.htm).\n",
      "\n",
      "Of course, later people realized that **_multi-layer_ perceptrons** are capable of learning the logic of an XOR gate, but they require something called **_backpropagation_** for the network to learn from trials and errors. After all, remember that deep learning neural networks are data-driven. If we have a model and its actual output is different from the **desired output,** we need a way to back-propagate the error information along the neural network to tell weights to adjust and correct themselves by a certain value so that gradually the actual output from the model gets closer to the desired output after rounds and rounds of testing.\n",
      "\n",
      "As it turns out, for more complicated tasks that involve outputs which cannot be produced from a **linear combination** of inputs (so the outputs are non-linear or not linearly separable), step function won’t work because it doesn’t support backpropagation, which require the chosen activation function to have meaningful derivative.\n",
      "\n",
      "![Image 21](https://miro.medium.com/v2/resize:fit:262/1*8HHgIgCkLTMzXeTj05YzOw.png)\n",
      "\n",
      "Source: mathnotes.org\n",
      "\n",
      "Some calculus talk: Step function is a **linear activation function** whose derivative is zero for all input points except the point zero. At point zero, the derivative is undefined since the function is discontinuous at point zero. So although it is a very simple and easy activation function, it cannot handle more complicated tasks. Keep reading and you’ll find out more.\n",
      "\n",
      "Linear vs. Non-linear\n",
      "---------------------\n",
      "\n",
      "What!? So what is a **linear combination**? And why can’t a perceptron learn an XOR gate? This is getting confusing now. No problem, here’s an explanation:\n",
      "\n",
      "![Image 22](https://miro.medium.com/v2/resize:fit:700/1*75NcLk9P6Gb4qg7CaSY66w.jpeg)\n",
      "\n",
      "Think for a moment about our previous example. With 3 binary inputs for whether you earn more money after learning DL, if the math and programming involved are easy or not, and if you can learn DL without investing in expensive hardware, we have a total of _2³ = 8_ possible sets of inputs and outcomes. With weights (_w1 = 6, w2 = 2, w3 = 2),_ and _bias = -5,_ we have the following table for a set of (_x1, x2, x3_):\n",
      "\n",
      "1.  (1, 0, 0) -\\> sum + bias = 6–5 = 1, desired output = 1\n",
      "2.  (1, 1, 0) -\\> sum + bias = 8–5 = 3, desired output = 1\n",
      "3.  (1, 1, 1) -\\> sum + bias = 10–5 = 5, desired output = 1\n",
      "4.  (1, 0, 1) -\\> sum + bias = 8–5 = 3, desired output = 1\n",
      "5.  (0, 1, 1) -\\> sum + bias = 4–5 = -1, desired output = 0\n",
      "6.  (0, 1, 0) -\\> sum + bias = 2–5 = -3, desired output = 0\n",
      "7.  (0, 0, 1) -\\> sum + bias = 2–5 = -3, desired output = 0\n",
      "8.  (0, 0, 0) -\\> sum + bias = 0–5 = -5, desired output = 0\n",
      "\n",
      "So literally, if we start with random weights than the ones of (_w1 = 6, w2 = 2, w3 = 2_), our perceptron will try to learn to adjust and find the ideal weights (_w1 = 6, w2 = 2, w3 = 2_), which would correctly match the actual output of each set of (_x1, x2, x3_) to the desired output. In fact, we can separate these 8 possible sets in a 3D space with a plane, such as the plane of _x1 = 0.5_. This type of classification problem where you can draw a plane to separate different outputs (or draw a line for outputs in 2D) is a problem that our single-layer perceptron can solve.\n",
      "\n",
      "![Image 23](https://miro.medium.com/v2/resize:fit:700/1*GvfX88bue3hI912P5XEISA.png)\n",
      "\n",
      "A plane separating the 4 sets to the left from the 4 to the right\n",
      "\n",
      "![Image 24](https://miro.medium.com/v2/resize:fit:700/1*iw4PaZhzE3koOVy-7VTjBQ.png)\n",
      "\n",
      "A Plane separating the 4 sets to the left from the 4 to the right\n",
      "\n",
      "I hope you can imagine the plane separating the 8 sets of inputs above. Since the sets of (_x1, x2, x3_) involves a 3-dimensional space, it can be a bit challenging. But in general, a single-layer perceptron with a linear activation function can learn to separate a dataset like the chart A in the graph below, which is separable by a line of _y = ax + b_. But if the dataset is only separable non-linearly by a circle like in chart B, our perceptron will perform horribly.\n",
      "\n",
      "![Image 25](https://miro.medium.com/v2/resize:fit:500/1*XPPtw4l43VTJHinqSqOkRg.png)\n",
      "\n",
      "Source: Sebastian Raschka\n",
      "\n",
      "Why is it so? I copy paste a quote from Stack Overflow for you. You can check out the answer [here](https://stackoverflow.com/a/35919708/6297414) as well.\n",
      "\n",
      "> “Activation functions cannot be linear because neural networks with a linear activation function are effective only one layer deep, regardless of how complex their architecture are. Input to networks are usually linear transformation (input \\* weight), but real world and problems are non-linear. To make the incoming data nonlinear, we use nonlinear mapping called activation function. An activation function is a decision making function that determines the presence of particular neural feature. It is mapped between 0 and 1, where zero mean the feature is not there, while one means the feature is present. Unfortunately, the small changes occurring in the weights cannot be reflected in the activation value because it can only take either 0 or 1. Therefore, nonlinear functions must be continuous and differentiable between this range. A neural network must be able to take any input from -infinity to +infinite, but it should be able to map it to an output that ranges between {0,1} or between {-1,1} in some cases — thus the need for activation function. Non-linearity is needed in activation functions because its aim in a neural network is to produce a nonlinear decision boundary via non-linear combinations of the weight and inputs.” -user7479\n",
      "\n",
      "So why can’t a single-layer perceptron with step function learn the XOR gate? Check out the graph below. On the left is an XOR logic chart, and on the right is a cartesian representation of the 2 different outcomes (1 and 0) from an XOR gate.\n",
      "\n",
      "![Image 26](https://miro.medium.com/v2/resize:fit:342/1*gDF_i5BXuutx-yR1aXqXNQ.gif)\n",
      "\n",
      "Source: [https://stackoverflow.com/a/35919708/6297414](https://stackoverflow.com/a/35919708/6297414)\n",
      "\n",
      "Indeed, we cannot possibly separate the white dots from the black dots with a single, linear line. We need to have something more powerful to let a neural network learn the XOR gate logic. I will write more on this topic soon.\n",
      "\n",
      "Recap\n",
      "-----\n",
      "\n",
      "Deep learning is an exciting field that is rapidly changing our society. We should care about deep learning and it is fun to understand at least the basics of it. We also introduced a very basic neural network called (single-layer) perceptron and learned about how the decision-making model of perceptron works.\n",
      "\n",
      "Our single-layer perceptron plus a step function works fine for simple linearly-separable binary classification. But that’s about it…as it won’t work very well for more complicated problems involving non-linear outputs. I know this sounds a bit disappointing, but we will cover them in detail next!\n",
      "\n",
      "Coming next will be a [new post](https://medium.com/towards-data-science/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f) on neural networks with **hidden layers** (sounds fancy right?) and a new activation function called the **sigmoid function**. If space allows, we will touch on **gradient descent** and **backpropagation,** which are the key concepts for a smart neural network to learn. Stay tuned ([here’s the link to the second post](https://medium.com/towards-data-science/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f)) :)\n",
      "\n",
      "For now, congratulations on following the post so far and you’ve already learned a lot now! Stay motivated and I hope you are having fun with deep learning! If you are impatient to wait for my writing, check out the following free resources to learn more already:\n",
      "\n",
      "Enjoy learning!\n",
      "\n",
      "_Did you enjoy this reading? Don’t forget to follow me on_ [_Twitter_](https://twitter.com/nahuakang)_!_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8526b943-22d3-462a-af1b-fba4896285d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    context:str = Field(description=\"the context served as a knowledge\")\n",
    "    message:str = Field(description=\"the user input message\")\n",
    "    answer:str = Field(description=\"the answer\", default=None)\n",
    "\n",
    "message=\"Please explain the concept of 'Deep Learning' in simple term, assume that I have zero knowledge.\"\n",
    "payload = Payload(context=text, message=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e580710-524c-49a9-8497-bc453226b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_agents.web_chat_agent import WebChatAgent\n",
    "wca = WebChatAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c720add-742f-411a-9215-5fe2e3697d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to explain the concept of 'Deep Learning' in simple terms.\n",
      "\n",
      "**What is Deep Learning?**\n",
      "\n",
      "Deep Learning is a type of Artificial Intelligence (AI) that allows computers to learn and improve on their own by analyzing large amounts of data. It's like teaching a child to recognize pictures of cats and dogs by showing them many examples.\n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "Imagine you have a big box of toys, and inside the box, there are many different types of toys, like blocks, dolls, and cars. A computer with Deep Learning can look at the toys and try to figure out what they are, just like a human would. But instead of using a simple rule, like \"if it's red, it's a ball,\" the computer uses a complex set of rules that it learns from the data.\n",
      "\n",
      "**What's special about Deep Learning?**\n",
      "\n",
      "Deep Learning is special because it can learn from large amounts of data, and it can improve its performance over time. It's like a child learning to ride a bike - at first, they might fall off, but with practice, they get better and better.\n",
      "\n",
      "**What can Deep Learning do?**\n",
      "\n",
      "Deep Learning can do many things, like:\n",
      "\n",
      "*   Recognize pictures and objects\n",
      "*   Understand speech and language\n",
      "*   Play games like chess and Go\n",
      "*   Drive cars and robots\n",
      "*   Help doctors diagnose diseases\n",
      "\n",
      "**Is Deep Learning the same as Artificial Intelligence?**\n",
      "\n",
      "No, Deep Learning is a type of Artificial Intelligence, but not all Artificial Intelligence is Deep Learning. Think of it like a big umbrella - Deep Learning is a specific type of AI that falls under the umbrella of AI.\n",
      "\n",
      "**Is Deep Learning easy to learn?**\n",
      "\n",
      "Deep Learning can be challenging to learn, but it's not impossible. It requires a lot of data, computational power, and expertise in machine learning and programming. But with practice and patience, anyone can learn the basics of Deep Learning.\n",
      "\n",
      "In summary, Deep Learning is a type of AI that allows computers to learn and improve on their own by analyzing large amounts of data. It's a complex field that requires expertise, but it has many exciting applications in areas like image recognition, speech recognition, and game playing.\n",
      "CPU times: user 34.4 ms, sys: 7.63 ms, total: 42 ms\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "answer = wca.run(payload)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f629fd-eebc-44b2-a3ea-772694dad536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broai-ra",
   "language": "python",
   "name": "broai-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
